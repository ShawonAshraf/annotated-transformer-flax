{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShawonAshraf/annotated-transformer-flax/blob/main/nb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxp4yFMqFjoL"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook is going to be an attempt to replicate the implementation of the transformer architecture : [Attention is all you need](https://arxiv.org/abs/1706.03762), as shown in the [Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/) notebook but using flax and jax transforms."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flax altair einops optax chex brax"
      ],
      "metadata": {
        "id": "L5lXGw37FmJ2",
        "outputId": "d418fa7e-c61a-45cc-9c6f-e8b8ecbf934b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flax in /usr/local/lib/python3.11/dist-packages (0.10.4)\n",
            "Requirement already satisfied: altair in /usr/local/lib/python3.11/dist-packages (5.5.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (0.2.4)\n",
            "Requirement already satisfied: chex in /usr/local/lib/python3.11/dist-packages (0.1.89)\n",
            "Collecting brax\n",
            "  Downloading brax-0.12.1-py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from flax) (2.0.2)\n",
            "Requirement already satisfied: jax>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from flax) (0.5.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax) (1.1.0)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from flax) (0.11.10)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax) (0.1.72)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.11/dist-packages (from flax) (13.9.4)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from flax) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax) (6.0.2)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from flax) (0.1.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair) (1.31.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from altair) (24.2)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from optax) (1.4.0)\n",
            "Requirement already satisfied: jaxlib>=0.4.27 in /usr/local/lib/python3.11/dist-packages (from optax) (0.5.1)\n",
            "Requirement already satisfied: etils[epy] in /usr/local/lib/python3.11/dist-packages (from optax) (1.12.2)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex) (0.12.1)\n",
            "Collecting dm_env (from brax)\n",
            "  Downloading dm_env-1.6-py3-none-any.whl.metadata (966 bytes)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (from brax) (3.1.0)\n",
            "Collecting flask_cors (from brax)\n",
            "  Downloading flask_cors-5.0.1-py3-none-any.whl.metadata (961 bytes)\n",
            "Requirement already satisfied: grpcio in /usr/local/lib/python3.11/dist-packages (from brax) (1.71.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (from brax) (0.25.2)\n",
            "Collecting jaxopt (from brax)\n",
            "  Downloading jaxopt-0.8.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting ml_collections (from brax)\n",
            "  Downloading ml_collections-1.0.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting mujoco (from brax)\n",
            "  Downloading mujoco-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m570.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mujoco-mjx (from brax)\n",
            "  Downloading mujoco_mjx-3.3.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from brax) (11.1.0)\n",
            "Collecting pytinyrenderer (from brax)\n",
            "  Downloading pytinyrenderer-0.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from brax) (1.14.1)\n",
            "Collecting tensorboardX (from brax)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting trimesh (from brax)\n",
            "  Downloading trimesh-4.6.5-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.27->flax) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.27->flax) (3.4.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair) (0.23.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax) (2.18.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from dm_env->brax) (0.1.9)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask->brax) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask->brax) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask->brax) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask->brax) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair) (3.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym->brax) (3.1.1)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym->brax) (0.0.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from ml_collections->brax) (1.17.0)\n",
            "Collecting glfw (from mujoco->brax)\n",
            "  Downloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyopengl in /usr/local/lib/python3.11/dist-packages (from mujoco->brax) (3.1.9)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax) (5.29.4)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax) (4.12.1)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax) (3.20.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.11/dist-packages (from dm-tree->dm_env->brax) (1.17.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epy]->optax) (2025.3.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[epy]->optax) (6.5.2)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epy]->optax) (3.21.0)\n",
            "Downloading brax-0.12.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dm_env-1.6-py3-none-any.whl (26 kB)\n",
            "Downloading flask_cors-5.0.1-py3-none-any.whl (11 kB)\n",
            "Downloading jaxopt-0.8.3-py3-none-any.whl (172 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_collections-1.0.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mujoco-3.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mujoco_mjx-3.3.0-py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytinyrenderer-0.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trimesh-4.6.5-py3-none-any.whl (708 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m708.9/708.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading glfw-2.8.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pytinyrenderer, glfw, trimesh, tensorboardX, ml_collections, dm_env, mujoco, flask_cors, mujoco-mjx, jaxopt, brax\n",
            "Successfully installed brax-0.12.1 dm_env-1.6 flask_cors-5.0.1 glfw-2.8.0 jaxopt-0.8.3 ml_collections-1.0.0 mujoco-3.3.0 mujoco-mjx-3.3.0 pytinyrenderer-0.0.14 tensorboardX-2.6.2.2 trimesh-4.6.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgRoia_UFjoU"
      },
      "source": [
        "## Preliminary Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keVyljBFFjoV"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import altair as alt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1C6qaK-PFjoW"
      },
      "outputs": [],
      "source": [
        "# jax master prng key\n",
        "master_key = jax.random.key(2025)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMS0415ZFjoW"
      },
      "source": [
        "## Transformer\n",
        "\n",
        "Transformer is a neural network architecture to model sequences using only self attention, which is a departure from prior sequence modelling techniques, which either relied on convolutions or recurrent methods.\n",
        "\n",
        "Both convolutions and recurrent methods have their drawbacks. To keep it short, convs can only attend a specific number of sequence elements at a time (window limit) and recurrent networks work sequentially, which make them slower. Self Attention removes both these limitations by parallely modelling the sequence against itself (gets rid of the window limit as well!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "2"
        },
        "id": "Yt_QwBuoFjoX"
      },
      "source": [
        "Initially introduced (and still used) for language modelling, Transformer is an Encoder-Decoder architecture at the core, with the following flow for a sequence of tokens (from a text sequence):\n",
        "\n",
        "![Flow Diagram for Transformer](https://github.com/ShawonAshraf/annotated-transformer-flax/blob/main/images/flow.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWVnG-iqFjoX"
      },
      "source": [
        "We'll start with the embedding part first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KCKNggmFjoX"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3PUh37wFjoX"
      },
      "outputs": [],
      "source": [
        "import flax.linen as nn\n",
        "\n",
        "class SequenceEmbedding(nn.Module):\n",
        "    d_model: int\n",
        "    vocab_size: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        embeddings = nn.Embed(num_embeddings=self.vocab_size, features=self.d_model)(x)\n",
        "        return embeddings / jnp.sqrt(self.d_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfJixYZ6FjoY"
      },
      "source": [
        "The embeddings are scaled by a factor of $\\frac{1}{\\sqrt{D_{model}}}$, where $D_{model}$ is the dimension of the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AY9BWMVQFjoY",
        "outputId": "dac5e48f-153c-467f-fecc-204c90b80539"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.00881531  0.01145858 -0.0442136  ... -0.03573872  0.03622499\n",
            "   0.06185519]\n",
            " [-0.02964733  0.01290533 -0.09276884 ...  0.06421088 -0.0477102\n",
            "   0.04761793]\n",
            " [ 0.02071862 -0.00026872  0.04888517 ...  0.05003741  0.06048984\n",
            "  -0.03646948]\n",
            " ...\n",
            " [-0.020246    0.0312751  -0.01027099 ...  0.0198579  -0.01917169\n",
            "   0.00367736]\n",
            " [ 0.11928734  0.03426114  0.05348063 ...  0.00294668 -0.00699746\n",
            "   0.04327941]\n",
            " [-0.00780651 -0.00029554  0.02707211 ... -0.0541172  -0.05628999\n",
            "  -0.01496975]]\n"
          ]
        }
      ],
      "source": [
        "embed_key, master_key = jax.random.split(master_key, 2)\n",
        "\n",
        "x = jnp.arange(0, 100)\n",
        "embed_layer = SequenceEmbedding(20, 100)\n",
        "vars = embed_layer.init(embed_key, x)\n",
        "embeddings = embed_layer.apply(vars, x)\n",
        "\n",
        "print(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_oHNLIHFjoZ"
      },
      "source": [
        "## Positional Encoding\n",
        "\n",
        "The original Transformer paper implemented an \"Absolute Position Encoding\". Transformer doesn't have recurrence. As a result, it lacks the sense of position in a sequence, (also self attention compares a sequence against itself and no positional awareness will make it learn random correlations between the elements or tokens)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwHoxvS_FjoZ"
      },
      "outputs": [],
      "source": [
        "from einops import rearrange\n",
        "\n",
        "\n",
        "class AbsolutePositionalEncoder(nn.Module):\n",
        "    d_model: int\n",
        "    max_len: int\n",
        "    dropout: float = 0.1\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        # encoding\n",
        "        encoding = np.zeros((self.max_len, self.d_model))\n",
        "        position = np.arange(0, self.max_len)\n",
        "        # must be in the shape max_len, 1\n",
        "        position = rearrange(position, \"max_len -> max_len 1\")\n",
        "\n",
        "        factor = np.exp(\n",
        "            np.arange(0, self.d_model, 2) *\n",
        "            (-np.log(np.array([1.0e4])) / self.d_model)\n",
        "        )\n",
        "\n",
        "        # encoding for odd and even positions\n",
        "        # even, 0::2\n",
        "        encoding[:, 0::2] = np.sin(position * factor)\n",
        "        # odd, 1::2\n",
        "        encoding[:, 1::2] = np.cos(position * factor)\n",
        "\n",
        "        # reshape\n",
        "        encoding = rearrange(encoding, \"s dmodel -> 1 s dmodel\")\n",
        "\n",
        "        encoded_x = x + encoding[:, : x.shape[1]]\n",
        "        # apply dropout\n",
        "        encoded_x = nn.Dropout(self.dropout, deterministic=True)(encoded_x)\n",
        "\n",
        "        return encoded_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gx6O9Ex5FjoZ"
      },
      "outputs": [],
      "source": [
        "pos_key, master_key = jax.random.split(master_key, 2)\n",
        "\n",
        "\n",
        "# input in the shape: 1, max_len, d_model\n",
        "x = jnp.zeros((1, 100, 20))\n",
        "\n",
        "encoder = AbsolutePositionalEncoder(20, 100, 0.0)\n",
        "vars = encoder.init(pos_key, x)\n",
        "encodings = encoder.apply(vars, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-I26vmxFjoa"
      },
      "outputs": [],
      "source": [
        "# using the same plotting function from the annotated transformer notebook\n",
        "\n",
        "def plot_encoding(y, max_len, dim_range):\n",
        "    data = pd.concat(\n",
        "        [\n",
        "            pd.DataFrame(\n",
        "                {\n",
        "                    \"embedding\": np.array(y)[0, :, dim],\n",
        "                    \"dimension\": dim,\n",
        "                    \"position\": list(range(max_len)),\n",
        "                }\n",
        "            )\n",
        "            for dim in dim_range\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        alt.Chart(data)\n",
        "        .mark_line()\n",
        "        .properties(width=800)\n",
        "        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n",
        "        .interactive()\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLRyf8BOFjoa"
      },
      "outputs": [],
      "source": [
        "chart = plot_encoding(encodings, 100, [4, 5, 6, 7])\n",
        "chart.save(\"enc.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88VUt1TuFjoa"
      },
      "source": [
        "![Positional Encoding non interactive](https://github.com/ShawonAshraf/annotated-transformer-flax/blob/main/enc.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpCb2H1rFjoa",
        "outputId": "38da8032-1cad-40e8-d520-54489c5aaaa5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "  #altair-viz-d840770cd5f24486805fa71a2a614700.vega-embed {\n",
              "    width: 100%;\n",
              "    display: flex;\n",
              "  }\n",
              "\n",
              "  #altair-viz-d840770cd5f24486805fa71a2a614700.vega-embed details,\n",
              "  #altair-viz-d840770cd5f24486805fa71a2a614700.vega-embed details summary {\n",
              "    position: relative;\n",
              "  }\n",
              "</style>\n",
              "<div id=\"altair-viz-d840770cd5f24486805fa71a2a614700\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-d840770cd5f24486805fa71a2a614700\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-d840770cd5f24486805fa71a2a614700\");\n",
              "    }\n",
              "\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      let deps = [\"vega-embed\"];\n",
              "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-464eebdfea7d9698f9e2e7e29a6c93c7\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"dimension\", \"type\": \"nominal\"}, \"x\": {\"field\": \"position\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"embedding\", \"type\": \"quantitative\"}}, \"params\": [{\"name\": \"param_1\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\"}, {\"name\": \"param_2\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\"}], \"width\": 800, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-464eebdfea7d9698f9e2e7e29a6c93c7\": [{\"embedding\": 0.0, \"dimension\": 4, \"position\": 0}, {\"embedding\": 0.15782664716243744, \"dimension\": 4, \"position\": 1}, {\"embedding\": 0.3116971552371979, \"dimension\": 4, \"position\": 2}, {\"embedding\": 0.45775455236434937, \"dimension\": 4, \"position\": 3}, {\"embedding\": 0.5923377275466919, \"dimension\": 4, \"position\": 4}, {\"embedding\": 0.7120731472969055, \"dimension\": 4, \"position\": 5}, {\"embedding\": 0.813959538936615, \"dimension\": 4, \"position\": 6}, {\"embedding\": 0.8954429626464844, \"dimension\": 4, \"position\": 7}, {\"embedding\": 0.9544808864593506, \"dimension\": 4, \"position\": 8}, {\"embedding\": 0.989593505859375, \"dimension\": 4, \"position\": 9}, {\"embedding\": 0.9999006390571594, \"dimension\": 4, \"position\": 10}, {\"embedding\": 0.9851439595222473, \"dimension\": 4, \"position\": 11}, {\"embedding\": 0.94569331407547, \"dimension\": 4, \"position\": 12}, {\"embedding\": 0.8825376033782959, \"dimension\": 4, \"position\": 13}, {\"embedding\": 0.7972599267959595, \"dimension\": 4, \"position\": 14}, {\"embedding\": 0.6919978260993958, \"dimension\": 4, \"position\": 15}, {\"embedding\": 0.5693899393081665, \"dimension\": 4, \"position\": 16}, {\"embedding\": 0.43250951170921326, \"dimension\": 4, \"position\": 17}, {\"embedding\": 0.284787654876709, \"dimension\": 4, \"position\": 18}, {\"embedding\": 0.12992723286151886, \"dimension\": 4, \"position\": 19}, {\"embedding\": -0.028189996257424355, \"dimension\": 4, \"position\": 20}, {\"embedding\": -0.18560060858726501, \"dimension\": 4, \"position\": 21}, {\"embedding\": -0.33835887908935547, \"dimension\": 4, \"position\": 22}, {\"embedding\": -0.4826357662677765, \"dimension\": 4, \"position\": 23}, {\"embedding\": -0.6148146986961365, \"dimension\": 4, \"position\": 24}, {\"embedding\": -0.731582522392273, \"dimension\": 4, \"position\": 25}, {\"embedding\": -0.8300122618675232, \"dimension\": 4, \"position\": 26}, {\"embedding\": -0.9076366424560547, \"dimension\": 4, \"position\": 27}, {\"embedding\": -0.9625098705291748, \"dimension\": 4, \"position\": 28}, {\"embedding\": -0.9932565093040466, \"dimension\": 4, \"position\": 29}, {\"embedding\": -0.9991058707237244, \"dimension\": 4, \"position\": 30}, {\"embedding\": -0.9799113273620605, \"dimension\": 4, \"position\": 31}, {\"embedding\": -0.9361540079116821, \"dimension\": 4, \"position\": 32}, {\"embedding\": -0.8689307570457458, \"dimension\": 4, \"position\": 33}, {\"embedding\": -0.7799265384674072, \"dimension\": 4, \"position\": 34}, {\"embedding\": -0.6713724732398987, \"dimension\": 4, \"position\": 35}, {\"embedding\": -0.5459895730018616, \"dimension\": 4, \"position\": 36}, {\"embedding\": -0.4069207012653351, \"dimension\": 4, \"position\": 37}, {\"embedding\": -0.25765180587768555, \"dimension\": 4, \"position\": 38}, {\"embedding\": -0.10192455351352692, \"dimension\": 4, \"position\": 39}, {\"embedding\": 0.05635758489370346, \"dimension\": 4, \"position\": 40}, {\"embedding\": 0.2132270485162735, \"dimension\": 4, \"position\": 41}, {\"embedding\": 0.3647516965866089, \"dimension\": 4, \"position\": 42}, {\"embedding\": 0.5071333646774292, \"dimension\": 4, \"position\": 43}, {\"embedding\": 0.6368030309677124, \"dimension\": 4, \"position\": 44}, {\"embedding\": 0.7505103945732117, \"dimension\": 4, \"position\": 45}, {\"embedding\": 0.8454052209854126, \"dimension\": 4, \"position\": 46}, {\"embedding\": 0.9191088676452637, \"dimension\": 4, \"position\": 47}, {\"embedding\": 0.9697737693786621, \"dimension\": 4, \"position\": 48}, {\"embedding\": 0.9961300492286682, \"dimension\": 4, \"position\": 49}, {\"embedding\": 0.9975169897079468, \"dimension\": 4, \"position\": 50}, {\"embedding\": 0.9738998413085938, \"dimension\": 4, \"position\": 51}, {\"embedding\": 0.9258706569671631, \"dimension\": 4, \"position\": 52}, {\"embedding\": 0.8546332120895386, \"dimension\": 4, \"position\": 53}, {\"embedding\": 0.7619733214378357, \"dimension\": 4, \"position\": 54}, {\"embedding\": 0.6502134799957275, \"dimension\": 4, \"position\": 55}, {\"embedding\": 0.522155225276947, \"dimension\": 4, \"position\": 56}, {\"embedding\": 0.38100844621658325, \"dimension\": 4, \"position\": 57}, {\"embedding\": 0.23031117022037506, \"dimension\": 4, \"position\": 58}, {\"embedding\": 0.07384085655212402, \"dimension\": 4, \"position\": 59}, {\"embedding\": -0.08448038250207901, \"dimension\": 4, \"position\": 60}, {\"embedding\": -0.24068400263786316, \"dimension\": 4, \"position\": 61}, {\"embedding\": -0.3908545672893524, \"dimension\": 4, \"position\": 62}, {\"embedding\": -0.5312278270721436, \"dimension\": 4, \"position\": 63}, {\"embedding\": -0.6582852005958557, \"dimension\": 4, \"position\": 64}, {\"embedding\": -0.7688417434692383, \"dimension\": 4, \"position\": 65}, {\"embedding\": -0.860126256942749, \"dimension\": 4, \"position\": 66}, {\"embedding\": -0.9298505187034607, \"dimension\": 4, \"position\": 67}, {\"embedding\": -0.9762669205665588, \"dimension\": 4, \"position\": 68}, {\"embedding\": -0.9982118606567383, \"dimension\": 4, \"position\": 69}, {\"embedding\": -0.9951352477073669, \"dimension\": 4, \"position\": 70}, {\"embedding\": -0.9671143293380737, \"dimension\": 4, \"position\": 71}, {\"embedding\": -0.9148513674736023, \"dimension\": 4, \"position\": 72}, {\"embedding\": -0.839656412601471, \"dimension\": 4, \"position\": 73}, {\"embedding\": -0.7434144020080566, \"dimension\": 4, \"position\": 74}, {\"embedding\": -0.6285377144813538, \"dimension\": 4, \"position\": 75}, {\"embedding\": -0.4979058504104614, \"dimension\": 4, \"position\": 76}, {\"embedding\": -0.35479333996772766, \"dimension\": 4, \"position\": 77}, {\"embedding\": -0.20278748869895935, \"dimension\": 4, \"position\": 78}, {\"embedding\": -0.04569847136735916, \"dimension\": 4, \"position\": 79}, {\"embedding\": 0.11253602802753448, \"dimension\": 4, \"position\": 80}, {\"embedding\": 0.2679496705532074, \"dimension\": 4, \"position\": 81}, {\"embedding\": 0.4166467785835266, \"dimension\": 4, \"position\": 82}, {\"embedding\": 0.5549001097679138, \"dimension\": 4, \"position\": 83}, {\"embedding\": 0.6792441010475159, \"dimension\": 4, \"position\": 84}, {\"embedding\": 0.7865619659423828, \"dimension\": 4, \"position\": 85}, {\"embedding\": 0.8741636276245117, \"dimension\": 4, \"position\": 86}, {\"embedding\": 0.9398531317710876, \"dimension\": 4, \"position\": 87}, {\"embedding\": 0.9819840788841248, \"dimension\": 4, \"position\": 88}, {\"embedding\": 0.9995002150535583, \"dimension\": 4, \"position\": 89}, {\"embedding\": 0.9919625520706177, \"dimension\": 4, \"position\": 90}, {\"embedding\": 0.9595600366592407, \"dimension\": 4, \"position\": 91}, {\"embedding\": 0.9031049013137817, \"dimension\": 4, \"position\": 92}, {\"embedding\": 0.8240122199058533, \"dimension\": 4, \"position\": 93}, {\"embedding\": 0.7242646217346191, \"dimension\": 4, \"position\": 94}, {\"embedding\": 0.6063622832298279, \"dimension\": 4, \"position\": 95}, {\"embedding\": 0.4732607305049896, \"dimension\": 4, \"position\": 96}, {\"embedding\": 0.3282962739467621, \"dimension\": 4, \"position\": 97}, {\"embedding\": 0.1751026064157486, \"dimension\": 4, \"position\": 98}, {\"embedding\": 0.017519766464829445, \"dimension\": 4, \"position\": 99}, {\"embedding\": 1.0, \"dimension\": 5, \"position\": 0}, {\"embedding\": 0.9874668121337891, \"dimension\": 5, \"position\": 1}, {\"embedding\": 0.9501814842224121, \"dimension\": 5, \"position\": 2}, {\"embedding\": 0.8890786170959473, \"dimension\": 5, \"position\": 3}, {\"embedding\": 0.8056897521018982, \"dimension\": 5, \"position\": 4}, {\"embedding\": 0.7021052837371826, \"dimension\": 5, \"position\": 5}, {\"embedding\": 0.5809215307235718, \"dimension\": 5, \"position\": 6}, {\"embedding\": 0.44517627358436584, \"dimension\": 5, \"position\": 7}, {\"embedding\": 0.298272043466568, \"dimension\": 5, \"position\": 8}, {\"embedding\": 0.14389123022556305, \"dimension\": 5, \"position\": 9}, {\"embedding\": -0.014096398837864399, \"dimension\": 5, \"position\": 10}, {\"embedding\": -0.17173068225383759, \"dimension\": 5, \"position\": 11}, {\"embedding\": -0.32506030797958374, \"dimension\": 5, \"position\": 12}, {\"embedding\": -0.47024187445640564, \"dimension\": 5, \"position\": 13}, {\"embedding\": -0.6036362051963806, \"dimension\": 5, \"position\": 14}, {\"embedding\": -0.7218995690345764, \"dimension\": 5, \"position\": 15}, {\"embedding\": -0.8220675587654114, \"dimension\": 5, \"position\": 16}, {\"embedding\": -0.9016293883323669, \"dimension\": 5, \"position\": 17}, {\"embedding\": -0.9585906267166138, \"dimension\": 5, \"position\": 18}, {\"embedding\": -0.9915235042572021, \"dimension\": 5, \"position\": 19}, {\"embedding\": -0.9996025562286377, \"dimension\": 5, \"position\": 20}, {\"embedding\": -0.9826252460479736, \"dimension\": 5, \"position\": 21}, {\"embedding\": -0.9410171508789062, \"dimension\": 5, \"position\": 22}, {\"embedding\": -0.8758211731910706, \"dimension\": 5, \"position\": 23}, {\"embedding\": -0.788671612739563, \"dimension\": 5, \"position\": 24}, {\"embedding\": -0.6817529201507568, \"dimension\": 5, \"position\": 25}, {\"embedding\": -0.5577451586723328, \"dimension\": 5, \"position\": 26}, {\"embedding\": -0.41975679993629456, \"dimension\": 5, \"position\": 27}, {\"embedding\": -0.27124670147895813, \"dimension\": 5, \"position\": 28}, {\"embedding\": -0.11593741178512573, \"dimension\": 5, \"position\": 29}, {\"embedding\": 0.042277991771698, \"dimension\": 5, \"position\": 30}, {\"embedding\": 0.19943363964557648, \"dimension\": 5, \"position\": 31}, {\"embedding\": 0.35159021615982056, \"dimension\": 5, \"position\": 32}, {\"embedding\": 0.49493372440338135, \"dimension\": 5, \"position\": 33}, {\"embedding\": 0.6258710622787476, \"dimension\": 5, \"position\": 34}, {\"embedding\": 0.7411201000213623, \"dimension\": 5, \"position\": 35}, {\"embedding\": 0.8377919793128967, \"dimension\": 5, \"position\": 36}, {\"embedding\": 0.9134634733200073, \"dimension\": 5, \"position\": 37}, {\"embedding\": 0.9662378430366516, \"dimension\": 5, \"position\": 38}, {\"embedding\": 0.994792103767395, \"dimension\": 5, \"position\": 39}, {\"embedding\": 0.9984106421470642, \"dimension\": 5, \"position\": 40}, {\"embedding\": 0.9770026803016663, \"dimension\": 5, \"position\": 41}, {\"embedding\": 0.931104838848114, \"dimension\": 5, \"position\": 42}, {\"embedding\": 0.8618676066398621, \"dimension\": 5, \"position\": 43}, {\"embedding\": 0.7710265517234802, \"dimension\": 5, \"position\": 44}, {\"embedding\": 0.6608586311340332, \"dimension\": 5, \"position\": 45}, {\"embedding\": 0.5341254472732544, \"dimension\": 5, \"position\": 46}, {\"embedding\": 0.3940037190914154, \"dimension\": 5, \"position\": 47}, {\"embedding\": 0.2440057396888733, \"dimension\": 5, \"position\": 48}, {\"embedding\": 0.08789143711328506, \"dimension\": 5, \"position\": 49}, {\"embedding\": -0.07042597979307175, \"dimension\": 5, \"position\": 50}, {\"embedding\": -0.22697807848453522, \"dimension\": 5, \"position\": 51}, {\"embedding\": -0.37784066796302795, \"dimension\": 5, \"position\": 52}, {\"embedding\": -0.5192321538925171, \"dimension\": 5, \"position\": 53}, {\"embedding\": -0.6476084589958191, \"dimension\": 5, \"position\": 54}, {\"embedding\": -0.759751558303833, \"dimension\": 5, \"position\": 55}, {\"embedding\": -0.8528504967689514, \"dimension\": 5, \"position\": 56}, {\"embedding\": -0.9245715737342834, \"dimension\": 5, \"position\": 57}, {\"embedding\": -0.9731170535087585, \"dimension\": 5, \"position\": 58}, {\"embedding\": -0.9972700476646423, \"dimension\": 5, \"position\": 59}, {\"embedding\": -0.9964251518249512, \"dimension\": 5, \"position\": 60}, {\"embedding\": -0.9706035256385803, \"dimension\": 5, \"position\": 61}, {\"embedding\": -0.9204524755477905, \"dimension\": 5, \"position\": 62}, {\"embedding\": -0.84722900390625, \"dimension\": 5, \"position\": 63}, {\"embedding\": -0.7527686357498169, \"dimension\": 5, \"position\": 64}, {\"embedding\": -0.6394391059875488, \"dimension\": 5, \"position\": 65}, {\"embedding\": -0.5100812315940857, \"dimension\": 5, \"position\": 66}, {\"embedding\": -0.36793744564056396, \"dimension\": 5, \"position\": 67}, {\"embedding\": -0.21657085418701172, \"dimension\": 5, \"position\": 68}, {\"embedding\": -0.05977560579776764, \"dimension\": 5, \"position\": 69}, {\"embedding\": 0.0985179916024208, \"dimension\": 5, \"position\": 70}, {\"embedding\": 0.25434210896492004, \"dimension\": 5, \"position\": 71}, {\"embedding\": 0.40379080176353455, \"dimension\": 5, \"position\": 72}, {\"embedding\": 0.5431179404258728, \"dimension\": 5, \"position\": 73}, {\"embedding\": 0.6688311100006104, \"dimension\": 5, \"position\": 74}, {\"embedding\": 0.7777791023254395, \"dimension\": 5, \"position\": 75}, {\"embedding\": 0.8672310709953308, \"dimension\": 5, \"position\": 76}, {\"embedding\": 0.934944748878479, \"dimension\": 5, \"position\": 77}, {\"embedding\": 0.9792227745056152, \"dimension\": 5, \"position\": 78}, {\"embedding\": 0.998955249786377, \"dimension\": 5, \"position\": 79}, {\"embedding\": 0.9936476349830627, \"dimension\": 5, \"position\": 80}, {\"embedding\": 0.9634329080581665, \"dimension\": 5, \"position\": 81}, {\"embedding\": 0.9090684652328491, \"dimension\": 5, \"position\": 82}, {\"embedding\": 0.8319169878959656, \"dimension\": 5, \"position\": 83}, {\"embedding\": 0.7339124083518982, \"dimension\": 5, \"position\": 84}, {\"embedding\": 0.6175113320350647, \"dimension\": 5, \"position\": 85}, {\"embedding\": 0.4856315553188324, \"dimension\": 5, \"position\": 86}, {\"embedding\": 0.34157875180244446, \"dimension\": 5, \"position\": 87}, {\"embedding\": 0.18896381556987762, \"dimension\": 5, \"position\": 88}, {\"embedding\": 0.03161226212978363, \"dimension\": 5, \"position\": 89}, {\"embedding\": -0.1265317052602768, \"dimension\": 5, \"position\": 90}, {\"embedding\": -0.28150397539138794, \"dimension\": 5, \"position\": 91}, {\"embedding\": -0.42941999435424805, \"dimension\": 5, \"position\": 92}, {\"embedding\": -0.5665720105171204, \"dimension\": 5, \"position\": 93}, {\"embedding\": -0.6895221471786499, \"dimension\": 5, \"position\": 94}, {\"embedding\": -0.7951884865760803, \"dimension\": 5, \"position\": 95}, {\"embedding\": -0.8809223771095276, \"dimension\": 5, \"position\": 96}, {\"embedding\": -0.9445748329162598, \"dimension\": 5, \"position\": 97}, {\"embedding\": -0.9845501780509949, \"dimension\": 5, \"position\": 98}, {\"embedding\": -0.9998465180397034, \"dimension\": 5, \"position\": 99}, {\"embedding\": 0.0, \"dimension\": 6, \"position\": 0}, {\"embedding\": 0.06305387616157532, \"dimension\": 6, \"position\": 1}, {\"embedding\": 0.12585681676864624, \"dimension\": 6, \"position\": 2}, {\"embedding\": 0.18815888464450836, \"dimension\": 6, \"position\": 3}, {\"embedding\": 0.24971210956573486, \"dimension\": 6, \"position\": 4}, {\"embedding\": 0.31027156114578247, \"dimension\": 6, \"position\": 5}, {\"embedding\": 0.3695961833000183, \"dimension\": 6, \"position\": 6}, {\"embedding\": 0.42744994163513184, \"dimension\": 6, \"position\": 7}, {\"embedding\": 0.48360252380371094, \"dimension\": 6, \"position\": 8}, {\"embedding\": 0.5378305315971375, \"dimension\": 6, \"position\": 9}, {\"embedding\": 0.5899180769920349, \"dimension\": 6, \"position\": 10}, {\"embedding\": 0.6396578550338745, \"dimension\": 6, \"position\": 11}, {\"embedding\": 0.686851978302002, \"dimension\": 6, \"position\": 12}, {\"embedding\": 0.73131263256073, \"dimension\": 6, \"position\": 13}, {\"embedding\": 0.7728628516197205, \"dimension\": 6, \"position\": 14}, {\"embedding\": 0.8113372325897217, \"dimension\": 6, \"position\": 15}, {\"embedding\": 0.8465827107429504, \"dimension\": 6, \"position\": 16}, {\"embedding\": 0.8784589767456055, \"dimension\": 6, \"position\": 17}, {\"embedding\": 0.9068392515182495, \"dimension\": 6, \"position\": 18}, {\"embedding\": 0.9316104650497437, \"dimension\": 6, \"position\": 19}, {\"embedding\": 0.952674150466919, \"dimension\": 6, \"position\": 20}, {\"embedding\": 0.9699463844299316, \"dimension\": 6, \"position\": 21}, {\"embedding\": 0.9833585023880005, \"dimension\": 6, \"position\": 22}, {\"embedding\": 0.9928570985794067, \"dimension\": 6, \"position\": 23}, {\"embedding\": 0.9984043836593628, \"dimension\": 6, \"position\": 24}, {\"embedding\": 0.999978244304657, \"dimension\": 6, \"position\": 25}, {\"embedding\": 0.9975724220275879, \"dimension\": 6, \"position\": 26}, {\"embedding\": 0.9911965727806091, \"dimension\": 6, \"position\": 27}, {\"embedding\": 0.9808759689331055, \"dimension\": 6, \"position\": 28}, {\"embedding\": 0.9666516780853271, \"dimension\": 6, \"position\": 29}, {\"embedding\": 0.9485804438591003, \"dimension\": 6, \"position\": 30}, {\"embedding\": 0.9267340302467346, \"dimension\": 6, \"position\": 31}, {\"embedding\": 0.901199460029602, \"dimension\": 6, \"position\": 32}, {\"embedding\": 0.8720783591270447, \"dimension\": 6, \"position\": 33}, {\"embedding\": 0.8394865989685059, \"dimension\": 6, \"position\": 34}, {\"embedding\": 0.8035538792610168, \"dimension\": 6, \"position\": 35}, {\"embedding\": 0.7644231915473938, \"dimension\": 6, \"position\": 36}, {\"embedding\": 0.7222503423690796, \"dimension\": 6, \"position\": 37}, {\"embedding\": 0.6772030591964722, \"dimension\": 6, \"position\": 38}, {\"embedding\": 0.6294607520103455, \"dimension\": 6, \"position\": 39}, {\"embedding\": 0.5792132616043091, \"dimension\": 6, \"position\": 40}, {\"embedding\": 0.5266607403755188, \"dimension\": 6, \"position\": 41}, {\"embedding\": 0.47201216220855713, \"dimension\": 6, \"position\": 42}, {\"embedding\": 0.41548511385917664, \"dimension\": 6, \"position\": 43}, {\"embedding\": 0.35730454325675964, \"dimension\": 6, \"position\": 44}, {\"embedding\": 0.29770198464393616, \"dimension\": 6, \"position\": 45}, {\"embedding\": 0.23691466450691223, \"dimension\": 6, \"position\": 46}, {\"embedding\": 0.1751844733953476, \"dimension\": 6, \"position\": 47}, {\"embedding\": 0.11275708675384521, \"dimension\": 6, \"position\": 48}, {\"embedding\": 0.04988095909357071, \"dimension\": 6, \"position\": 49}, {\"embedding\": -0.013193685561418533, \"dimension\": 6, \"position\": 50}, {\"embedding\": -0.07621581852436066, \"dimension\": 6, \"position\": 51}, {\"embedding\": -0.1389346420764923, \"dimension\": 6, \"position\": 52}, {\"embedding\": -0.20110052824020386, \"dimension\": 6, \"position\": 53}, {\"embedding\": -0.26246610283851624, \"dimension\": 6, \"position\": 54}, {\"embedding\": -0.3227871060371399, \"dimension\": 6, \"position\": 55}, {\"embedding\": -0.38182350993156433, \"dimension\": 6, \"position\": 56}, {\"embedding\": -0.4393403232097626, \"dimension\": 6, \"position\": 57}, {\"embedding\": -0.4951086938381195, \"dimension\": 6, \"position\": 58}, {\"embedding\": -0.548906683921814, \"dimension\": 6, \"position\": 59}, {\"embedding\": -0.600520133972168, \"dimension\": 6, \"position\": 60}, {\"embedding\": -0.6497436165809631, \"dimension\": 6, \"position\": 61}, {\"embedding\": -0.6963813304901123, \"dimension\": 6, \"position\": 62}, {\"embedding\": -0.7402476668357849, \"dimension\": 6, \"position\": 63}, {\"embedding\": -0.7811679244041443, \"dimension\": 6, \"position\": 64}, {\"embedding\": -0.8189793825149536, \"dimension\": 6, \"position\": 65}, {\"embedding\": -0.8535314798355103, \"dimension\": 6, \"position\": 66}, {\"embedding\": -0.8846867084503174, \"dimension\": 6, \"position\": 67}, {\"embedding\": -0.912321150302887, \"dimension\": 6, \"position\": 68}, {\"embedding\": -0.9363247156143188, \"dimension\": 6, \"position\": 69}, {\"embedding\": -0.9566020369529724, \"dimension\": 6, \"position\": 70}, {\"embedding\": -0.9730722308158875, \"dimension\": 6, \"position\": 71}, {\"embedding\": -0.9856699109077454, \"dimension\": 6, \"position\": 72}, {\"embedding\": -0.9943448305130005, \"dimension\": 6, \"position\": 73}, {\"embedding\": -0.9990624785423279, \"dimension\": 6, \"position\": 74}, {\"embedding\": -0.9998041391372681, \"dimension\": 6, \"position\": 75}, {\"embedding\": -0.9965668320655823, \"dimension\": 6, \"position\": 76}, {\"embedding\": -0.989363431930542, \"dimension\": 6, \"position\": 77}, {\"embedding\": -0.9782226085662842, \"dimension\": 6, \"position\": 78}, {\"embedding\": -0.9631887078285217, \"dimension\": 6, \"position\": 79}, {\"embedding\": -0.9443215727806091, \"dimension\": 6, \"position\": 80}, {\"embedding\": -0.9216962456703186, \"dimension\": 6, \"position\": 81}, {\"embedding\": -0.8954028487205505, \"dimension\": 6, \"position\": 82}, {\"embedding\": -0.865545928478241, \"dimension\": 6, \"position\": 83}, {\"embedding\": -0.832244336605072, \"dimension\": 6, \"position\": 84}, {\"embedding\": -0.7956306338310242, \"dimension\": 6, \"position\": 85}, {\"embedding\": -0.7558504939079285, \"dimension\": 6, \"position\": 86}, {\"embedding\": -0.7130622863769531, \"dimension\": 6, \"position\": 87}, {\"embedding\": -0.6674363017082214, \"dimension\": 6, \"position\": 88}, {\"embedding\": -0.6191540360450745, \"dimension\": 6, \"position\": 89}, {\"embedding\": -0.5684077143669128, \"dimension\": 6, \"position\": 90}, {\"embedding\": -0.5153992176055908, \"dimension\": 6, \"position\": 91}, {\"embedding\": -0.46033963561058044, \"dimension\": 6, \"position\": 92}, {\"embedding\": -0.4034479856491089, \"dimension\": 6, \"position\": 93}, {\"embedding\": -0.34495070576667786, \"dimension\": 6, \"position\": 94}, {\"embedding\": -0.28508061170578003, \"dimension\": 6, \"position\": 95}, {\"embedding\": -0.22407597303390503, \"dimension\": 6, \"position\": 96}, {\"embedding\": -0.16217957437038422, \"dimension\": 6, \"position\": 97}, {\"embedding\": -0.09963773190975189, \"dimension\": 6, \"position\": 98}, {\"embedding\": -0.03669935464859009, \"dimension\": 6, \"position\": 99}, {\"embedding\": 1.0, \"dimension\": 7, \"position\": 0}, {\"embedding\": 0.9980100989341736, \"dimension\": 7, \"position\": 1}, {\"embedding\": 0.992048442363739, \"dimension\": 7, \"position\": 2}, {\"embedding\": 0.9821385741233826, \"dimension\": 7, \"position\": 3}, {\"embedding\": 0.9683201313018799, \"dimension\": 7, \"position\": 4}, {\"embedding\": 0.9506479501724243, \"dimension\": 7, \"position\": 5}, {\"embedding\": 0.9291924834251404, \"dimension\": 7, \"position\": 6}, {\"embedding\": 0.9040390253067017, \"dimension\": 7, \"position\": 7}, {\"embedding\": 0.8752877116203308, \"dimension\": 7, \"position\": 8}, {\"embedding\": 0.8430529832839966, \"dimension\": 7, \"position\": 9}, {\"embedding\": 0.8074631094932556, \"dimension\": 7, \"position\": 10}, {\"embedding\": 0.768659770488739, \"dimension\": 7, \"position\": 11}, {\"embedding\": 0.7267972826957703, \"dimension\": 7, \"position\": 12}, {\"embedding\": 0.6820423603057861, \"dimension\": 7, \"position\": 13}, {\"embedding\": 0.6345731019973755, \"dimension\": 7, \"position\": 14}, {\"embedding\": 0.5845783948898315, \"dimension\": 7, \"position\": 15}, {\"embedding\": 0.5322571992874146, \"dimension\": 7, \"position\": 16}, {\"embedding\": 0.47781774401664734, \"dimension\": 7, \"position\": 17}, {\"embedding\": 0.42147669196128845, \"dimension\": 7, \"position\": 18}, {\"embedding\": 0.3634582757949829, \"dimension\": 7, \"position\": 19}, {\"embedding\": 0.3039934039115906, \"dimension\": 7, \"position\": 20}, {\"embedding\": 0.24331867694854736, \"dimension\": 7, \"position\": 21}, {\"embedding\": 0.18167562782764435, \"dimension\": 7, \"position\": 22}, {\"embedding\": 0.11930955201387405, \"dimension\": 7, \"position\": 23}, {\"embedding\": 0.056468646973371506, \"dimension\": 7, \"position\": 24}, {\"embedding\": -0.006596986670047045, \"dimension\": 7, \"position\": 25}, {\"embedding\": -0.06963636726140976, \"dimension\": 7, \"position\": 26}, {\"embedding\": -0.1323986053466797, \"dimension\": 7, \"position\": 27}, {\"embedding\": -0.19463394582271576, \"dimension\": 7, \"position\": 28}, {\"embedding\": -0.25609466433525085, \"dimension\": 7, \"position\": 29}, {\"embedding\": -0.31653621792793274, \"dimension\": 7, \"position\": 30}, {\"embedding\": -0.37571802735328674, \"dimension\": 7, \"position\": 31}, {\"embedding\": -0.4334045648574829, \"dimension\": 7, \"position\": 32}, {\"embedding\": -0.4893662631511688, \"dimension\": 7, \"position\": 33}, {\"embedding\": -0.5433804392814636, \"dimension\": 7, \"position\": 34}, {\"embedding\": -0.5952320098876953, \"dimension\": 7, \"position\": 35}, {\"embedding\": -0.6447147727012634, \"dimension\": 7, \"position\": 36}, {\"embedding\": -0.6916317343711853, \"dimension\": 7, \"position\": 37}, {\"embedding\": -0.7357961535453796, \"dimension\": 7, \"position\": 38}, {\"embedding\": -0.7770323157310486, \"dimension\": 7, \"position\": 39}, {\"embedding\": -0.8151760101318359, \"dimension\": 7, \"position\": 40}, {\"embedding\": -0.8500756025314331, \"dimension\": 7, \"position\": 41}, {\"embedding\": -0.8815920352935791, \"dimension\": 7, \"position\": 42}, {\"embedding\": -0.9095999598503113, \"dimension\": 7, \"position\": 43}, {\"embedding\": -0.9339879155158997, \"dimension\": 7, \"position\": 44}, {\"embedding\": -0.9546588659286499, \"dimension\": 7, \"position\": 45}, {\"embedding\": -0.9715304374694824, \"dimension\": 7, \"position\": 46}, {\"embedding\": -0.9845356345176697, \"dimension\": 7, \"position\": 47}, {\"embedding\": -0.9936226010322571, \"dimension\": 7, \"position\": 48}, {\"embedding\": -0.998755156993866, \"dimension\": 7, \"position\": 49}, {\"embedding\": -0.9999129772186279, \"dimension\": 7, \"position\": 50}, {\"embedding\": -0.9970913529396057, \"dimension\": 7, \"position\": 51}, {\"embedding\": -0.9903015494346619, \"dimension\": 7, \"position\": 52}, {\"embedding\": -0.9795706272125244, \"dimension\": 7, \"position\": 53}, {\"embedding\": -0.9649412035942078, \"dimension\": 7, \"position\": 54}, {\"embedding\": -0.9464715719223022, \"dimension\": 7, \"position\": 55}, {\"embedding\": -0.9242352843284607, \"dimension\": 7, \"position\": 56}, {\"embedding\": -0.8983206748962402, \"dimension\": 7, \"position\": 57}, {\"embedding\": -0.8688310384750366, \"dimension\": 7, \"position\": 58}, {\"embedding\": -0.835883617401123, \"dimension\": 7, \"position\": 59}, {\"embedding\": -0.7996096611022949, \"dimension\": 7, \"position\": 60}, {\"embedding\": -0.7601534128189087, \"dimension\": 7, \"position\": 61}, {\"embedding\": -0.7176719307899475, \"dimension\": 7, \"position\": 62}, {\"embedding\": -0.6723343133926392, \"dimension\": 7, \"position\": 63}, {\"embedding\": -0.6243209838867188, \"dimension\": 7, \"position\": 64}, {\"embedding\": -0.5738229751586914, \"dimension\": 7, \"position\": 65}, {\"embedding\": -0.5210413336753845, \"dimension\": 7, \"position\": 66}, {\"embedding\": -0.4661860466003418, \"dimension\": 7, \"position\": 67}, {\"embedding\": -0.4094754755496979, \"dimension\": 7, \"position\": 68}, {\"embedding\": -0.3511352837085724, \"dimension\": 7, \"position\": 69}, {\"embedding\": -0.29139766097068787, \"dimension\": 7, \"position\": 70}, {\"embedding\": -0.23050034046173096, \"dimension\": 7, \"position\": 71}, {\"embedding\": -0.1686856895685196, \"dimension\": 7, \"position\": 72}, {\"embedding\": -0.1061997190117836, \"dimension\": 7, \"position\": 73}, {\"embedding\": -0.04329109936952591, \"dimension\": 7, \"position\": 74}, {\"embedding\": 0.019789811223745346, \"dimension\": 7, \"position\": 75}, {\"embedding\": 0.08279196172952652, \"dimension\": 7, \"position\": 76}, {\"embedding\": 0.14546461403369904, \"dimension\": 7, \"position\": 77}, {\"embedding\": 0.20755836367607117, \"dimension\": 7, \"position\": 78}, {\"embedding\": 0.26882606744766235, \"dimension\": 7, \"position\": 79}, {\"embedding\": 0.32902392745018005, \"dimension\": 7, \"position\": 80}, {\"embedding\": 0.3879123628139496, \"dimension\": 7, \"position\": 81}, {\"embedding\": 0.4452569782733917, \"dimension\": 7, \"position\": 82}, {\"embedding\": 0.5008295774459839, \"dimension\": 7, \"position\": 83}, {\"embedding\": 0.5544090270996094, \"dimension\": 7, \"position\": 84}, {\"embedding\": 0.6057820916175842, \"dimension\": 7, \"position\": 85}, {\"embedding\": 0.6547442078590393, \"dimension\": 7, \"position\": 86}, {\"embedding\": 0.7011006474494934, \"dimension\": 7, \"position\": 87}, {\"embedding\": 0.7446669340133667, \"dimension\": 7, \"position\": 88}, {\"embedding\": 0.785269558429718, \"dimension\": 7, \"position\": 89}, {\"embedding\": 0.8227470517158508, \"dimension\": 7, \"position\": 90}, {\"embedding\": 0.8569501638412476, \"dimension\": 7, \"position\": 91}, {\"embedding\": 0.8877428770065308, \"dimension\": 7, \"position\": 92}, {\"embedding\": 0.9150025844573975, \"dimension\": 7, \"position\": 93}, {\"embedding\": 0.9386208057403564, \"dimension\": 7, \"position\": 94}, {\"embedding\": 0.9585035443305969, \"dimension\": 7, \"position\": 95}, {\"embedding\": 0.974571704864502, \"dimension\": 7, \"position\": 96}, {\"embedding\": 0.9867612719535828, \"dimension\": 7, \"position\": 97}, {\"embedding\": 0.995023787021637, \"dimension\": 7, \"position\": 98}, {\"embedding\": 0.9993263483047485, \"dimension\": 7, \"position\": 99}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chart.interactive()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b49gwRTFjob"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "The original transformer consisted of 6 encoder and decoder layers. Each encoder layer has 2 parts: the Multi Head Attention layer and the feed forward layer. The outputs from the layer are fed through an additional Residual (additive) and LayerNorm layer.\n",
        "\n",
        "![Encoder flow diagram](https://github.com/ShawonAshraf/annotated-transformer-flax/blob/main/images/encoder.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fuF4yluFjob"
      },
      "source": [
        "## Multi Head Attention\n",
        "\n",
        "A multi head attention layer consists of $n$ self-attention layers (also known as heads). Each head is expected to model a different pattern or aspect from a sequence.\n",
        "\n",
        "### Self-Attention\n",
        "\n",
        "Self-attention compares a sequence against itself and tries to figure out how each element correlates to the rest of the components (learning semantic similarities for example). As a result, for a sequence of length $N$, this operation becomes an $N \\times N$ comparison. Also, the self-attention has quadriatic memory complexity (hence all the context length limit and high gpu requirements in today's large language models).\n",
        "\n",
        "Self-attention uses query, key and values to describe its operation, which can get a bit confusing without proper semantics. To keep things simpler, consider that you're searching for information in a sequence. The query is your question or search input, the key is the available information in the sequence (consider the sequence as your search space of context of information) and values are the probability of each element in the sequence being the desired search result.\n",
        "\n",
        "(Latent spaces in the neural nets are just compressed search spaces, if you see it like that!)\n",
        "\n",
        "So for query $Q$, key $K$ and values $V$, attention $A$ is\n",
        "\n",
        "$$\n",
        "A(Q, K, V) = softmax(QK^{T})V\n",
        "$$\n",
        "\n",
        "Why softmax? V stores the probabilities. And softmax gives you a probability distribution.\n",
        "\n",
        "There's one small problem though, remember scaling the embeddings? It was done to avoid numerical overflow (quite common if your gradients suddenly explode during training). So, self-attention in transformer is also scaled by the a factor, which here is the dimension of the keys or the information space (consider this as narrowing your search space for speed).\n",
        "\n",
        "$$\n",
        "A(Q, K, V) = softmax(\\frac{QK^{T}}{\\sqrt{D_{key}}})V\n",
        "$$\n",
        "\n",
        "In a nutshell:\n",
        "\n",
        "![self attention block diagram](https://github.com/ShawonAshraf/annotated-transformer-flax/blob/main/images/attn.png?raw=1)\n",
        "\n",
        "\n",
        "For GPT like causal (or generative) models, which consist only of decoders (no encoders there!) there's a concept called masking. (Will get back to this later duding the decoder implementation!)\n",
        "\n",
        "![self attention block diagram with masking](https://github.com/ShawonAshraf/annotated-transformer-flax/blob/main/images/attn_mask.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USSNEw-EFjob"
      },
      "outputs": [],
      "source": [
        "def self_attention(q, k, v, mask=None, dropout=None):\n",
        "    d_key = q.shape[-1]\n",
        "\n",
        "    q_k_t = jnp.matmul(q, rearrange(k, \"batch h seq k -> batch h k seq\"))\n",
        "\n",
        "    scaled = q_k_t / jnp.sqrt(d_key)\n",
        "\n",
        "    if mask:\n",
        "        # since jax numpy doesn't have a masked fill like np.ma.array\n",
        "        # https://github.com/jax-ml/jax/discussions/9363#discussioncomment-2066105\n",
        "        scaled = jnp.where(mask == 0, -1e9, x)\n",
        "\n",
        "\n",
        "    proba = nn.softmax(scaled)\n",
        "    if dropout:\n",
        "        proba = dropout(proba)\n",
        "\n",
        "    attn = jnp.matmul(proba, v)\n",
        "    return attn\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk-Vz-MKFjob"
      },
      "source": [
        "Also, why mat-mul? Faster than scaled products and can be acclelerated on GPUs and TPUs (via XLA and CUDA).\n",
        "\n",
        "Okay, now that there's one attention head defined, we can have a look at Multi-Head-Attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk8myaCeFjoc"
      },
      "source": [
        "### Attention Heads\n",
        "\n",
        "![multi head attention block](https://github.com/ShawonAshraf/annotated-transformer-flax/blob/main/images/mha.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4fwffNsFjoc"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    nheads: int\n",
        "    d_model: int\n",
        "    dropout_value:float = 0.1\n",
        "\n",
        "\n",
        "    def setup(self):\n",
        "        assert self.d_model % self.nheads == 0, \"Number of heads must be a multiple of d_model\"\n",
        "\n",
        "        self.d_k = self.d_model // self.nheads\n",
        "\n",
        "        # 3 dense linear layers, for q, k, v\n",
        "        self.linear_layers = [nn.Dense(self.d_model)] * 3\n",
        "        self.dropout = nn.Dropout(self.dropout_value, deterministic=True)\n",
        "\n",
        "        # one final dense layer\n",
        "        self.final_linear = nn.Dense(self.d_model)\n",
        "\n",
        "    def __call__(self, q, k, v, mask=None):\n",
        "        if mask:\n",
        "            mask = rearrange(mask, \"s -> s 1\")\n",
        "\n",
        "        # apply the dense layers\n",
        "        q, k, v = [\n",
        "            rearrange(lin(i), \"batch seq (h k) -> batch h seq k\", h=self.nheads, k=self.d_k)\n",
        "            for lin, i in zip(self.linear_layers, (q, k, v))\n",
        "        ]\n",
        "\n",
        "\n",
        "        # attention\n",
        "        score = self_attention(q, k, v, dropout=self.dropout, mask=mask)\n",
        "\n",
        "        # concat the scores\n",
        "        concatenated = jnp.concat(score)\n",
        "\n",
        "        # pass through a linear layer\n",
        "        return self.final_linear(concatenated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zc-6DJR3Fjoc",
        "outputId": "cdb9b100-7b45-46fe-9925-1f78e0b877e4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4, 100, 20)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mha_key, master_key = jax.random.split(master_key, 2)\n",
        "\n",
        "mha = MultiHeadAttention(4, 20)\n",
        "vars = mha.init(\n",
        "    mha_key,\n",
        "    encodings,\n",
        "    encodings,\n",
        "    encodings\n",
        ")\n",
        "\n",
        "mha_out = mha.apply(\n",
        "    vars,\n",
        "    encodings,\n",
        "    encodings,\n",
        "    encodings\n",
        ")\n",
        "\n",
        "mha_out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8mAHPZoFjod"
      },
      "source": [
        "(A note to the people who wrote the updated version of the annotated transformer notebook: were you writing for an audience or just revising for an exam? Things which can be said can always be said in easier words - same goes for code, don't overly complicate variable naming and function signatures.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-Ln4iBXFjod"
      },
      "source": [
        "### Layer Normalisation\n",
        "\n",
        "When you're dealing with sequences, they can be of variable lengths. And for this, the outputs (or activations) from a layer can have different sizes. So, you can't normalise them as batch or as a group. Layer norm can handle variable length outputs from a neural network layer. (P.S. layer norm doesn't perform well for fixed sized inputs, you're better off with batch norm or group norm)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rwUR73NFjod"
      },
      "outputs": [],
      "source": [
        "# you know, the pytorch docs were more helpful to figure this one out\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\n",
        "\n",
        "# TODO: fix layer norm\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    normalised_shape: tuple\n",
        "    eps: float = 1e-6\n",
        "\n",
        "    def setup(self):\n",
        "        # keeping bias 0\n",
        "        self.gamma = jnp.ones(shape=self.normalised_shape)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        mean = x.mean(axis=-1, keepdims=True)\n",
        "        std = x.std(axis=-1, keepdims=True)\n",
        "        norm = self.gamma * (x - mean) / (std + self.eps)\n",
        "\n",
        "        return norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7dqqqHLFjod"
      },
      "source": [
        "### Residual + Norm Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBBeMRHNFjod"
      },
      "outputs": [],
      "source": [
        "class AddAndNormLayer(nn.Module):\n",
        "    normalised_shape: tuple\n",
        "    dropout_rate: float\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        return x + LayerNorm(self.normalised_shape)(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji2UCgBaFjoe",
        "outputId": "878f3480-6d61-4ff5-fbf8-54d94bae280e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4, 100, 20)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "norm_res_key, master_key = jax.random.split(master_key, 2)\n",
        "\n",
        "add_norm_layer = AddAndNormLayer(mha_out.shape, 0.1)\n",
        "vars = add_norm_layer.init(norm_res_key, mha_out)\n",
        "add_norm_out = add_norm_layer.apply(vars, mha_out)\n",
        "add_norm_out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9Sw4hrVFjoe"
      },
      "source": [
        "### Feed Forward Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HD3nqzcRFjoe"
      },
      "outputs": [],
      "source": [
        "class FeedForwardLayer(nn.Module):\n",
        "    d_model: int\n",
        "    hidden: int\n",
        "    dropout_rate: float = 0.1\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = nn.Dense(self.d_model, self.hidden)(x)\n",
        "        x = nn.Dense(self.hidden, self.d_model)(x)\n",
        "        x = nn.Dropout(self.dropout_rate, deterministic=True)(x)\n",
        "        # leaky_relu can be used as well!\n",
        "        x = nn.relu(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRpFj2CMFjof",
        "outputId": "9b28de2a-d0a9-4530-d13e-974eb4330a83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4, 100, 10)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ff_key, master_key = jax.random.split(master_key, 2)\n",
        "\n",
        "ff_layer = FeedForwardLayer(20, 10)\n",
        "vars = ff_layer.init(ff_key, add_norm_out)\n",
        "ff_out = ff_layer.apply(vars, add_norm_out)\n",
        "\n",
        "ff_out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRId3nsnFjog"
      },
      "source": [
        "### Gathering the encoder layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYwzOssHFjog"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    d_model: int\n",
        "    ff_hidden: int\n",
        "    dropout_rate: float = 0.1\n",
        "    nheads: int = 4\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = MultiHeadAttention(self.nheads, self.d_model, self.dropout_rate)(x, x, x)\n",
        "\n",
        "        x = AddAndNormLayer(x.shape, self.dropout_rate)(x)\n",
        "\n",
        "        x = FeedForwardLayer(self.d_model, self.ff_hidden, self.dropout_rate)(x)\n",
        "\n",
        "        x = AddAndNormLayer(x.shape, self.dropout_rate)(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qm0rgzThFjog",
        "outputId": "c59da085-0e04-413f-ee7f-03e31db29872"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4, 100, 80)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transformer_encoder = TransformerEncoder(d_model=20, ff_hidden=80)\n",
        "\n",
        "enc_key, master_key = jax.random.split(master_key, 2)\n",
        "vars = transformer_encoder.init(enc_key, encodings)\n",
        "enc_out = transformer_encoder.apply(vars, encodings)\n",
        "enc_out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzxE5GQuFjoh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}