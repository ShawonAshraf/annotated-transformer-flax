{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShawonAshraf/annotated-transformer-flax/blob/main/nb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxp4yFMqFjoL"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook is going to be an attempt to replicate the implementation of the transformer architecture : [Attention is all you need](https://arxiv.org/abs/1706.03762), as shown in the [Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/) notebook but using flax and jax transforms."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q flax altair einops optax chex brax"
      ],
      "metadata": {
        "id": "L5lXGw37FmJ2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgRoia_UFjoU"
      },
      "source": [
        "## Preliminary Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "keVyljBFFjoV"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import altair as alt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1C6qaK-PFjoW"
      },
      "outputs": [],
      "source": [
        "# jax master prng key\n",
        "master_key = jax.random.key(2025)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMS0415ZFjoW"
      },
      "source": [
        "## Transformer\n",
        "\n",
        "Transformer is a neural network architecture to model sequences using only self attention, which is a departure from prior sequence modelling techniques, which either relied on convolutions or recurrent methods.\n",
        "\n",
        "Both convolutions and recurrent methods have their drawbacks. To keep it short, convs can only attend a specific number of sequence elements at a time (window limit) and recurrent networks work sequentially, which make them slower. Self Attention removes both these limitations by parallely modelling the sequence against itself (gets rid of the window limit as well!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "notebookRunGroups": {
          "groupValue": "2"
        },
        "id": "Yt_QwBuoFjoX"
      },
      "source": [
        "Initially introduced (and still used) for language modelling, Transformer is an Encoder-Decoder architecture at the core, with the following flow for a sequence of tokens (from a text sequence):\n",
        "\n",
        "![Flow Diagram for Transformer](https://github.com/ShawonAshraf/annotated-transformer-flax/blob/main/images/flow.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWVnG-iqFjoX"
      },
      "source": [
        "We'll start with the embedding part first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KCKNggmFjoX"
      },
      "source": [
        "## Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "x3PUh37wFjoX"
      },
      "outputs": [],
      "source": [
        "import flax.linen as nn\n",
        "\n",
        "class SequenceEmbedding(nn.Module):\n",
        "    d_model: int\n",
        "    vocab_size: int\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        embeddings = nn.Embed(num_embeddings=self.vocab_size, features=self.d_model)(x)\n",
        "        return embeddings / jnp.sqrt(self.d_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfJixYZ6FjoY"
      },
      "source": [
        "The embeddings are scaled by a factor of $\\frac{1}{\\sqrt{D_{model}}}$, where $D_{model}$ is the dimension of the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AY9BWMVQFjoY",
        "outputId": "04fedc4d-719c-47e8-bea2-2bfe18dc60a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 20)\n"
          ]
        }
      ],
      "source": [
        "embed_key, master_key = jax.random.split(master_key, 2)\n",
        "\n",
        "x = jnp.arange(0, 100)\n",
        "embed_layer = SequenceEmbedding(20, 100)\n",
        "vars = embed_layer.init(embed_key, x)\n",
        "embeddings = embed_layer.apply(vars, x)\n",
        "\n",
        "print(embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_oHNLIHFjoZ"
      },
      "source": [
        "## Positional Encoding\n",
        "\n",
        "The original Transformer paper implemented an \"Absolute Position Encoding\". Transformer doesn't have recurrence. As a result, it lacks the sense of position in a sequence, (also self attention compares a sequence against itself and no positional awareness will make it learn random correlations between the elements or tokens)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UwHoxvS_FjoZ"
      },
      "outputs": [],
      "source": [
        "from einops import rearrange\n",
        "\n",
        "\n",
        "class AbsolutePositionalEncoder(nn.Module):\n",
        "    d_model: int\n",
        "    max_len: int\n",
        "    dropout: float = 0.1\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        # encoding\n",
        "        encoding = np.zeros((self.max_len, self.d_model))\n",
        "        position = np.arange(0, self.max_len)\n",
        "        # must be in the shape max_len, 1\n",
        "        position = rearrange(position, \"max_len -> max_len 1\")\n",
        "\n",
        "        factor = np.exp(\n",
        "            np.arange(0, self.d_model, 2) *\n",
        "            (-np.log(np.array([1.0e4])) / self.d_model)\n",
        "        )\n",
        "\n",
        "        # encoding for odd and even positions\n",
        "        # even, 0::2\n",
        "        encoding[:, 0::2] = np.sin(position * factor)\n",
        "        # odd, 1::2\n",
        "        encoding[:, 1::2] = np.cos(position * factor)\n",
        "\n",
        "        # reshape\n",
        "        encoding = rearrange(encoding, \"s dmodel -> 1 s dmodel\")\n",
        "\n",
        "        encoded_x = x + encoding[:, : x.shape[1]]\n",
        "        # apply dropout\n",
        "        encoded_x = nn.Dropout(self.dropout, deterministic=True)(encoded_x)\n",
        "\n",
        "        return encoded_x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gx6O9Ex5FjoZ"
      },
      "outputs": [],
      "source": [
        "pos_key, master_key = jax.random.split(master_key, 2)\n",
        "\n",
        "\n",
        "# input in the shape: 1, max_len, d_model\n",
        "x = jnp.zeros((1, 100, 20))\n",
        "\n",
        "encoder = AbsolutePositionalEncoder(20, 100, 0.0)\n",
        "vars = encoder.init(pos_key, x)\n",
        "encodings = encoder.apply(vars, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "w-I26vmxFjoa"
      },
      "outputs": [],
      "source": [
        "# using the same plotting function from the annotated transformer notebook\n",
        "\n",
        "def plot_encoding(y, max_len, dim_range):\n",
        "    data = pd.concat(\n",
        "        [\n",
        "            pd.DataFrame(\n",
        "                {\n",
        "                    \"embedding\": np.array(y)[0, :, dim],\n",
        "                    \"dimension\": dim,\n",
        "                    \"position\": list(range(max_len)),\n",
        "                }\n",
        "            )\n",
        "            for dim in dim_range\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        alt.Chart(data)\n",
        "        .mark_line()\n",
        "        .properties(width=800)\n",
        "        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n",
        "        .interactive()\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KLRyf8BOFjoa"
      },
      "outputs": [],
      "source": [
        "chart = plot_encoding(encodings, 100, [4, 5, 6, 7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "wpCb2H1rFjoa",
        "outputId": "fd26557f-47f6-4f3e-9c51-d7793f4c98a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "  #altair-viz-355e05fee23a48f59a307170493d5514.vega-embed {\n",
              "    width: 100%;\n",
              "    display: flex;\n",
              "  }\n",
              "\n",
              "  #altair-viz-355e05fee23a48f59a307170493d5514.vega-embed details,\n",
              "  #altair-viz-355e05fee23a48f59a307170493d5514.vega-embed details summary {\n",
              "    position: relative;\n",
              "  }\n",
              "</style>\n",
              "<div id=\"altair-viz-355e05fee23a48f59a307170493d5514\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-355e05fee23a48f59a307170493d5514\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-355e05fee23a48f59a307170493d5514\");\n",
              "    }\n",
              "\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      let deps = [\"vega-embed\"];\n",
              "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-464eebdfea7d9698f9e2e7e29a6c93c7\"}, \"mark\": {\"type\": \"line\"}, \"encoding\": {\"color\": {\"field\": \"dimension\", \"type\": \"nominal\"}, \"x\": {\"field\": \"position\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"embedding\", \"type\": \"quantitative\"}}, \"params\": [{\"name\": \"param_2\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\"}, {\"name\": \"param_3\", \"select\": {\"type\": \"interval\", \"encodings\": [\"x\", \"y\"]}, \"bind\": \"scales\"}], \"width\": 800, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-464eebdfea7d9698f9e2e7e29a6c93c7\": [{\"embedding\": 0.0, \"dimension\": 4, \"position\": 0}, {\"embedding\": 0.15782664716243744, \"dimension\": 4, \"position\": 1}, {\"embedding\": 0.3116971552371979, \"dimension\": 4, \"position\": 2}, {\"embedding\": 0.45775455236434937, \"dimension\": 4, \"position\": 3}, {\"embedding\": 0.5923377275466919, \"dimension\": 4, \"position\": 4}, {\"embedding\": 0.7120731472969055, \"dimension\": 4, \"position\": 5}, {\"embedding\": 0.813959538936615, \"dimension\": 4, \"position\": 6}, {\"embedding\": 0.8954429626464844, \"dimension\": 4, \"position\": 7}, {\"embedding\": 0.9544808864593506, \"dimension\": 4, \"position\": 8}, {\"embedding\": 0.989593505859375, \"dimension\": 4, \"position\": 9}, {\"embedding\": 0.9999006390571594, \"dimension\": 4, \"position\": 10}, {\"embedding\": 0.9851439595222473, \"dimension\": 4, \"position\": 11}, {\"embedding\": 0.94569331407547, \"dimension\": 4, \"position\": 12}, {\"embedding\": 0.8825376033782959, \"dimension\": 4, \"position\": 13}, {\"embedding\": 0.7972599267959595, \"dimension\": 4, \"position\": 14}, {\"embedding\": 0.6919978260993958, \"dimension\": 4, \"position\": 15}, {\"embedding\": 0.5693899393081665, \"dimension\": 4, \"position\": 16}, {\"embedding\": 0.43250951170921326, \"dimension\": 4, \"position\": 17}, {\"embedding\": 0.284787654876709, \"dimension\": 4, \"position\": 18}, {\"embedding\": 0.12992723286151886, \"dimension\": 4, \"position\": 19}, {\"embedding\": -0.028189996257424355, \"dimension\": 4, \"position\": 20}, {\"embedding\": -0.18560060858726501, \"dimension\": 4, \"position\": 21}, {\"embedding\": -0.33835887908935547, \"dimension\": 4, \"position\": 22}, {\"embedding\": -0.4826357662677765, \"dimension\": 4, \"position\": 23}, {\"embedding\": -0.6148146986961365, \"dimension\": 4, \"position\": 24}, {\"embedding\": -0.731582522392273, \"dimension\": 4, \"position\": 25}, {\"embedding\": -0.8300122618675232, \"dimension\": 4, \"position\": 26}, {\"embedding\": -0.9076366424560547, \"dimension\": 4, \"position\": 27}, {\"embedding\": -0.9625098705291748, \"dimension\": 4, \"position\": 28}, {\"embedding\": -0.9932565093040466, \"dimension\": 4, \"position\": 29}, {\"embedding\": -0.9991058707237244, \"dimension\": 4, \"position\": 30}, {\"embedding\": -0.9799113273620605, \"dimension\": 4, \"position\": 31}, {\"embedding\": -0.9361540079116821, \"dimension\": 4, \"position\": 32}, {\"embedding\": -0.8689307570457458, \"dimension\": 4, \"position\": 33}, {\"embedding\": -0.7799265384674072, \"dimension\": 4, \"position\": 34}, {\"embedding\": -0.6713724732398987, \"dimension\": 4, \"position\": 35}, {\"embedding\": -0.5459895730018616, \"dimension\": 4, \"position\": 36}, {\"embedding\": -0.4069207012653351, \"dimension\": 4, \"position\": 37}, {\"embedding\": -0.25765180587768555, \"dimension\": 4, \"position\": 38}, {\"embedding\": -0.10192455351352692, \"dimension\": 4, \"position\": 39}, {\"embedding\": 0.05635758489370346, \"dimension\": 4, \"position\": 40}, {\"embedding\": 0.2132270485162735, \"dimension\": 4, \"position\": 41}, {\"embedding\": 0.3647516965866089, \"dimension\": 4, \"position\": 42}, {\"embedding\": 0.5071333646774292, \"dimension\": 4, \"position\": 43}, {\"embedding\": 0.6368030309677124, \"dimension\": 4, \"position\": 44}, {\"embedding\": 0.7505103945732117, \"dimension\": 4, \"position\": 45}, {\"embedding\": 0.8454052209854126, \"dimension\": 4, \"position\": 46}, {\"embedding\": 0.9191088676452637, \"dimension\": 4, \"position\": 47}, {\"embedding\": 0.9697737693786621, \"dimension\": 4, \"position\": 48}, {\"embedding\": 0.9961300492286682, \"dimension\": 4, \"position\": 49}, {\"embedding\": 0.9975169897079468, \"dimension\": 4, \"position\": 50}, {\"embedding\": 0.9738998413085938, \"dimension\": 4, \"position\": 51}, {\"embedding\": 0.9258706569671631, \"dimension\": 4, \"position\": 52}, {\"embedding\": 0.8546332120895386, \"dimension\": 4, \"position\": 53}, {\"embedding\": 0.7619733214378357, \"dimension\": 4, \"position\": 54}, {\"embedding\": 0.6502134799957275, \"dimension\": 4, \"position\": 55}, {\"embedding\": 0.522155225276947, \"dimension\": 4, \"position\": 56}, {\"embedding\": 0.38100844621658325, \"dimension\": 4, \"position\": 57}, {\"embedding\": 0.23031117022037506, \"dimension\": 4, \"position\": 58}, {\"embedding\": 0.07384085655212402, \"dimension\": 4, \"position\": 59}, {\"embedding\": -0.08448038250207901, \"dimension\": 4, \"position\": 60}, {\"embedding\": -0.24068400263786316, \"dimension\": 4, \"position\": 61}, {\"embedding\": -0.3908545672893524, \"dimension\": 4, \"position\": 62}, {\"embedding\": -0.5312278270721436, \"dimension\": 4, \"position\": 63}, {\"embedding\": -0.6582852005958557, \"dimension\": 4, \"position\": 64}, {\"embedding\": -0.7688417434692383, \"dimension\": 4, \"position\": 65}, {\"embedding\": -0.860126256942749, \"dimension\": 4, \"position\": 66}, {\"embedding\": -0.9298505187034607, \"dimension\": 4, \"position\": 67}, {\"embedding\": -0.9762669205665588, \"dimension\": 4, \"position\": 68}, {\"embedding\": -0.9982118606567383, \"dimension\": 4, \"position\": 69}, {\"embedding\": -0.9951352477073669, \"dimension\": 4, \"position\": 70}, {\"embedding\": -0.9671143293380737, \"dimension\": 4, \"position\": 71}, {\"embedding\": -0.9148513674736023, \"dimension\": 4, \"position\": 72}, {\"embedding\": -0.839656412601471, \"dimension\": 4, \"position\": 73}, {\"embedding\": -0.7434144020080566, \"dimension\": 4, \"position\": 74}, {\"embedding\": -0.6285377144813538, \"dimension\": 4, \"position\": 75}, {\"embedding\": -0.4979058504104614, \"dimension\": 4, \"position\": 76}, {\"embedding\": -0.35479333996772766, \"dimension\": 4, \"position\": 77}, {\"embedding\": -0.20278748869895935, \"dimension\": 4, \"position\": 78}, {\"embedding\": -0.04569847136735916, \"dimension\": 4, \"position\": 79}, {\"embedding\": 0.11253602802753448, \"dimension\": 4, \"position\": 80}, {\"embedding\": 0.2679496705532074, \"dimension\": 4, \"position\": 81}, {\"embedding\": 0.4166467785835266, \"dimension\": 4, \"position\": 82}, {\"embedding\": 0.5549001097679138, \"dimension\": 4, \"position\": 83}, {\"embedding\": 0.6792441010475159, \"dimension\": 4, \"position\": 84}, {\"embedding\": 0.7865619659423828, \"dimension\": 4, \"position\": 85}, {\"embedding\": 0.8741636276245117, \"dimension\": 4, \"position\": 86}, {\"embedding\": 0.9398531317710876, \"dimension\": 4, \"position\": 87}, {\"embedding\": 0.9819840788841248, \"dimension\": 4, \"position\": 88}, {\"embedding\": 0.9995002150535583, \"dimension\": 4, \"position\": 89}, {\"embedding\": 0.9919625520706177, \"dimension\": 4, \"position\": 90}, {\"embedding\": 0.9595600366592407, \"dimension\": 4, \"position\": 91}, {\"embedding\": 0.9031049013137817, \"dimension\": 4, \"position\": 92}, {\"embedding\": 0.8240122199058533, \"dimension\": 4, \"position\": 93}, {\"embedding\": 0.7242646217346191, \"dimension\": 4, \"position\": 94}, {\"embedding\": 0.6063622832298279, \"dimension\": 4, \"position\": 95}, {\"embedding\": 0.4732607305049896, \"dimension\": 4, \"position\": 96}, {\"embedding\": 0.3282962739467621, \"dimension\": 4, \"position\": 97}, {\"embedding\": 0.1751026064157486, \"dimension\": 4, \"position\": 98}, {\"embedding\": 0.017519766464829445, \"dimension\": 4, \"position\": 99}, {\"embedding\": 1.0, \"dimension\": 5, \"position\": 0}, {\"embedding\": 0.9874668121337891, \"dimension\": 5, \"position\": 1}, {\"embedding\": 0.9501814842224121, \"dimension\": 5, \"position\": 2}, {\"embedding\": 0.8890786170959473, \"dimension\": 5, \"position\": 3}, {\"embedding\": 0.8056897521018982, \"dimension\": 5, \"position\": 4}, {\"embedding\": 0.7021052837371826, \"dimension\": 5, \"position\": 5}, {\"embedding\": 0.5809215307235718, \"dimension\": 5, \"position\": 6}, {\"embedding\": 0.44517627358436584, \"dimension\": 5, \"position\": 7}, {\"embedding\": 0.298272043466568, \"dimension\": 5, \"position\": 8}, {\"embedding\": 0.14389123022556305, \"dimension\": 5, \"position\": 9}, {\"embedding\": -0.014096398837864399, \"dimension\": 5, \"position\": 10}, {\"embedding\": -0.17173068225383759, \"dimension\": 5, \"position\": 11}, {\"embedding\": -0.32506030797958374, \"dimension\": 5, \"position\": 12}, {\"embedding\": -0.47024187445640564, \"dimension\": 5, \"position\": 13}, {\"embedding\": -0.6036362051963806, \"dimension\": 5, \"position\": 14}, {\"embedding\": -0.7218995690345764, \"dimension\": 5, \"position\": 15}, {\"embedding\": -0.8220675587654114, \"dimension\": 5, \"position\": 16}, {\"embedding\": -0.9016293883323669, \"dimension\": 5, \"position\": 17}, {\"embedding\": -0.9585906267166138, \"dimension\": 5, \"position\": 18}, {\"embedding\": -0.9915235042572021, \"dimension\": 5, \"position\": 19}, {\"embedding\": -0.9996025562286377, \"dimension\": 5, \"position\": 20}, {\"embedding\": -0.9826252460479736, \"dimension\": 5, \"position\": 21}, {\"embedding\": -0.9410171508789062, \"dimension\": 5, \"position\": 22}, {\"embedding\": -0.8758211731910706, \"dimension\": 5, \"position\": 23}, {\"embedding\": -0.788671612739563, \"dimension\": 5, \"position\": 24}, {\"embedding\": -0.6817529201507568, \"dimension\": 5, \"position\": 25}, {\"embedding\": -0.5577451586723328, \"dimension\": 5, \"position\": 26}, {\"embedding\": -0.41975679993629456, \"dimension\": 5, \"position\": 27}, {\"embedding\": -0.27124670147895813, \"dimension\": 5, \"position\": 28}, {\"embedding\": -0.11593741178512573, \"dimension\": 5, \"position\": 29}, {\"embedding\": 0.042277991771698, \"dimension\": 5, \"position\": 30}, {\"embedding\": 0.19943363964557648, \"dimension\": 5, \"position\": 31}, {\"embedding\": 0.35159021615982056, \"dimension\": 5, \"position\": 32}, {\"embedding\": 0.49493372440338135, \"dimension\": 5, \"position\": 33}, {\"embedding\": 0.6258710622787476, \"dimension\": 5, \"position\": 34}, {\"embedding\": 0.7411201000213623, \"dimension\": 5, \"position\": 35}, {\"embedding\": 0.8377919793128967, \"dimension\": 5, \"position\": 36}, {\"embedding\": 0.9134634733200073, \"dimension\": 5, \"position\": 37}, {\"embedding\": 0.9662378430366516, \"dimension\": 5, \"position\": 38}, {\"embedding\": 0.994792103767395, \"dimension\": 5, \"position\": 39}, {\"embedding\": 0.9984106421470642, \"dimension\": 5, \"position\": 40}, {\"embedding\": 0.9770026803016663, \"dimension\": 5, \"position\": 41}, {\"embedding\": 0.931104838848114, \"dimension\": 5, \"position\": 42}, {\"embedding\": 0.8618676066398621, \"dimension\": 5, \"position\": 43}, {\"embedding\": 0.7710265517234802, \"dimension\": 5, \"position\": 44}, {\"embedding\": 0.6608586311340332, \"dimension\": 5, \"position\": 45}, {\"embedding\": 0.5341254472732544, \"dimension\": 5, \"position\": 46}, {\"embedding\": 0.3940037190914154, \"dimension\": 5, \"position\": 47}, {\"embedding\": 0.2440057396888733, \"dimension\": 5, \"position\": 48}, {\"embedding\": 0.08789143711328506, \"dimension\": 5, \"position\": 49}, {\"embedding\": -0.07042597979307175, \"dimension\": 5, \"position\": 50}, {\"embedding\": -0.22697807848453522, \"dimension\": 5, \"position\": 51}, {\"embedding\": -0.37784066796302795, \"dimension\": 5, \"position\": 52}, {\"embedding\": -0.5192321538925171, \"dimension\": 5, \"position\": 53}, {\"embedding\": -0.6476084589958191, \"dimension\": 5, \"position\": 54}, {\"embedding\": -0.759751558303833, \"dimension\": 5, \"position\": 55}, {\"embedding\": -0.8528504967689514, \"dimension\": 5, \"position\": 56}, {\"embedding\": -0.9245715737342834, \"dimension\": 5, \"position\": 57}, {\"embedding\": -0.9731170535087585, \"dimension\": 5, \"position\": 58}, {\"embedding\": -0.9972700476646423, \"dimension\": 5, \"position\": 59}, {\"embedding\": -0.9964251518249512, \"dimension\": 5, \"position\": 60}, {\"embedding\": -0.9706035256385803, \"dimension\": 5, \"position\": 61}, {\"embedding\": -0.9204524755477905, \"dimension\": 5, \"position\": 62}, {\"embedding\": -0.84722900390625, \"dimension\": 5, \"position\": 63}, {\"embedding\": -0.7527686357498169, \"dimension\": 5, \"position\": 64}, {\"embedding\": -0.6394391059875488, \"dimension\": 5, \"position\": 65}, {\"embedding\": -0.5100812315940857, \"dimension\": 5, \"position\": 66}, {\"embedding\": -0.36793744564056396, \"dimension\": 5, \"position\": 67}, {\"embedding\": -0.21657085418701172, \"dimension\": 5, \"position\": 68}, {\"embedding\": -0.05977560579776764, \"dimension\": 5, \"position\": 69}, {\"embedding\": 0.0985179916024208, \"dimension\": 5, \"position\": 70}, {\"embedding\": 0.25434210896492004, \"dimension\": 5, \"position\": 71}, {\"embedding\": 0.40379080176353455, \"dimension\": 5, \"position\": 72}, {\"embedding\": 0.5431179404258728, \"dimension\": 5, \"position\": 73}, {\"embedding\": 0.6688311100006104, \"dimension\": 5, \"position\": 74}, {\"embedding\": 0.7777791023254395, \"dimension\": 5, \"position\": 75}, {\"embedding\": 0.8672310709953308, \"dimension\": 5, \"position\": 76}, {\"embedding\": 0.934944748878479, \"dimension\": 5, \"position\": 77}, {\"embedding\": 0.9792227745056152, \"dimension\": 5, \"position\": 78}, {\"embedding\": 0.998955249786377, \"dimension\": 5, \"position\": 79}, {\"embedding\": 0.9936476349830627, \"dimension\": 5, \"position\": 80}, {\"embedding\": 0.9634329080581665, \"dimension\": 5, \"position\": 81}, {\"embedding\": 0.9090684652328491, \"dimension\": 5, \"position\": 82}, {\"embedding\": 0.8319169878959656, \"dimension\": 5, \"position\": 83}, {\"embedding\": 0.7339124083518982, \"dimension\": 5, \"position\": 84}, {\"embedding\": 0.6175113320350647, \"dimension\": 5, \"position\": 85}, {\"embedding\": 0.4856315553188324, \"dimension\": 5, \"position\": 86}, {\"embedding\": 0.34157875180244446, \"dimension\": 5, \"position\": 87}, {\"embedding\": 0.18896381556987762, \"dimension\": 5, \"position\": 88}, {\"embedding\": 0.03161226212978363, \"dimension\": 5, \"position\": 89}, {\"embedding\": -0.1265317052602768, \"dimension\": 5, \"position\": 90}, {\"embedding\": -0.28150397539138794, \"dimension\": 5, \"position\": 91}, {\"embedding\": -0.42941999435424805, \"dimension\": 5, \"position\": 92}, {\"embedding\": -0.5665720105171204, \"dimension\": 5, \"position\": 93}, {\"embedding\": -0.6895221471786499, \"dimension\": 5, \"position\": 94}, {\"embedding\": -0.7951884865760803, \"dimension\": 5, \"position\": 95}, {\"embedding\": -0.8809223771095276, \"dimension\": 5, \"position\": 96}, {\"embedding\": -0.9445748329162598, \"dimension\": 5, \"position\": 97}, {\"embedding\": -0.9845501780509949, \"dimension\": 5, \"position\": 98}, {\"embedding\": -0.9998465180397034, \"dimension\": 5, \"position\": 99}, {\"embedding\": 0.0, \"dimension\": 6, \"position\": 0}, {\"embedding\": 0.06305387616157532, \"dimension\": 6, \"position\": 1}, {\"embedding\": 0.12585681676864624, \"dimension\": 6, \"position\": 2}, {\"embedding\": 0.18815888464450836, \"dimension\": 6, \"position\": 3}, {\"embedding\": 0.24971210956573486, \"dimension\": 6, \"position\": 4}, {\"embedding\": 0.31027156114578247, \"dimension\": 6, \"position\": 5}, {\"embedding\": 0.3695961833000183, \"dimension\": 6, \"position\": 6}, {\"embedding\": 0.42744994163513184, \"dimension\": 6, \"position\": 7}, {\"embedding\": 0.48360252380371094, \"dimension\": 6, \"position\": 8}, {\"embedding\": 0.5378305315971375, \"dimension\": 6, \"position\": 9}, {\"embedding\": 0.5899180769920349, \"dimension\": 6, \"position\": 10}, {\"embedding\": 0.6396578550338745, \"dimension\": 6, \"position\": 11}, {\"embedding\": 0.686851978302002, \"dimension\": 6, \"position\": 12}, {\"embedding\": 0.73131263256073, \"dimension\": 6, \"position\": 13}, {\"embedding\": 0.7728628516197205, \"dimension\": 6, \"position\": 14}, {\"embedding\": 0.8113372325897217, \"dimension\": 6, \"position\": 15}, {\"embedding\": 0.8465827107429504, \"dimension\": 6, \"position\": 16}, {\"embedding\": 0.8784589767456055, \"dimension\": 6, \"position\": 17}, {\"embedding\": 0.9068392515182495, \"dimension\": 6, \"position\": 18}, {\"embedding\": 0.9316104650497437, \"dimension\": 6, \"position\": 19}, {\"embedding\": 0.952674150466919, \"dimension\": 6, \"position\": 20}, {\"embedding\": 0.9699463844299316, \"dimension\": 6, \"position\": 21}, {\"embedding\": 0.9833585023880005, \"dimension\": 6, \"position\": 22}, {\"embedding\": 0.9928570985794067, \"dimension\": 6, \"position\": 23}, {\"embedding\": 0.9984043836593628, \"dimension\": 6, \"position\": 24}, {\"embedding\": 0.999978244304657, \"dimension\": 6, \"position\": 25}, {\"embedding\": 0.9975724220275879, \"dimension\": 6, \"position\": 26}, {\"embedding\": 0.9911965727806091, \"dimension\": 6, \"position\": 27}, {\"embedding\": 0.9808759689331055, \"dimension\": 6, \"position\": 28}, {\"embedding\": 0.9666516780853271, \"dimension\": 6, \"position\": 29}, {\"embedding\": 0.9485804438591003, \"dimension\": 6, \"position\": 30}, {\"embedding\": 0.9267340302467346, \"dimension\": 6, \"position\": 31}, {\"embedding\": 0.901199460029602, \"dimension\": 6, \"position\": 32}, {\"embedding\": 0.8720783591270447, \"dimension\": 6, \"position\": 33}, {\"embedding\": 0.8394865989685059, \"dimension\": 6, \"position\": 34}, {\"embedding\": 0.8035538792610168, \"dimension\": 6, \"position\": 35}, {\"embedding\": 0.7644231915473938, \"dimension\": 6, \"position\": 36}, {\"embedding\": 0.7222503423690796, \"dimension\": 6, \"position\": 37}, {\"embedding\": 0.6772030591964722, \"dimension\": 6, \"position\": 38}, {\"embedding\": 0.6294607520103455, \"dimension\": 6, \"position\": 39}, {\"embedding\": 0.5792132616043091, \"dimension\": 6, \"position\": 40}, {\"embedding\": 0.5266607403755188, \"dimension\": 6, \"position\": 41}, {\"embedding\": 0.47201216220855713, \"dimension\": 6, \"position\": 42}, {\"embedding\": 0.41548511385917664, \"dimension\": 6, \"position\": 43}, {\"embedding\": 0.35730454325675964, \"dimension\": 6, \"position\": 44}, {\"embedding\": 0.29770198464393616, \"dimension\": 6, \"position\": 45}, {\"embedding\": 0.23691466450691223, \"dimension\": 6, \"position\": 46}, {\"embedding\": 0.1751844733953476, \"dimension\": 6, \"position\": 47}, {\"embedding\": 0.11275708675384521, \"dimension\": 6, \"position\": 48}, {\"embedding\": 0.04988095909357071, \"dimension\": 6, \"position\": 49}, {\"embedding\": -0.013193685561418533, \"dimension\": 6, \"position\": 50}, {\"embedding\": -0.07621581852436066, \"dimension\": 6, \"position\": 51}, {\"embedding\": -0.1389346420764923, \"dimension\": 6, \"position\": 52}, {\"embedding\": -0.20110052824020386, \"dimension\": 6, \"position\": 53}, {\"embedding\": -0.26246610283851624, \"dimension\": 6, \"position\": 54}, {\"embedding\": -0.3227871060371399, \"dimension\": 6, \"position\": 55}, {\"embedding\": -0.38182350993156433, \"dimension\": 6, \"position\": 56}, {\"embedding\": -0.4393403232097626, \"dimension\": 6, \"position\": 57}, {\"embedding\": -0.4951086938381195, \"dimension\": 6, \"position\": 58}, {\"embedding\": -0.548906683921814, \"dimension\": 6, \"position\": 59}, {\"embedding\": -0.600520133972168, \"dimension\": 6, \"position\": 60}, {\"embedding\": -0.6497436165809631, \"dimension\": 6, \"position\": 61}, {\"embedding\": -0.6963813304901123, \"dimension\": 6, \"position\": 62}, {\"embedding\": -0.7402476668357849, \"dimension\": 6, \"position\": 63}, {\"embedding\": -0.7811679244041443, \"dimension\": 6, \"position\": 64}, {\"embedding\": -0.8189793825149536, \"dimension\": 6, \"position\": 65}, {\"embedding\": -0.8535314798355103, \"dimension\": 6, \"position\": 66}, {\"embedding\": -0.8846867084503174, \"dimension\": 6, \"position\": 67}, {\"embedding\": -0.912321150302887, \"dimension\": 6, \"position\": 68}, {\"embedding\": -0.9363247156143188, \"dimension\": 6, \"position\": 69}, {\"embedding\": -0.9566020369529724, \"dimension\": 6, \"position\": 70}, {\"embedding\": -0.9730722308158875, \"dimension\": 6, \"position\": 71}, {\"embedding\": -0.9856699109077454, \"dimension\": 6, \"position\": 72}, {\"embedding\": -0.9943448305130005, \"dimension\": 6, \"position\": 73}, {\"embedding\": -0.9990624785423279, \"dimension\": 6, \"position\": 74}, {\"embedding\": -0.9998041391372681, \"dimension\": 6, \"position\": 75}, {\"embedding\": -0.9965668320655823, \"dimension\": 6, \"position\": 76}, {\"embedding\": -0.989363431930542, \"dimension\": 6, \"position\": 77}, {\"embedding\": -0.9782226085662842, \"dimension\": 6, \"position\": 78}, {\"embedding\": -0.9631887078285217, \"dimension\": 6, \"position\": 79}, {\"embedding\": -0.9443215727806091, \"dimension\": 6, \"position\": 80}, {\"embedding\": -0.9216962456703186, \"dimension\": 6, \"position\": 81}, {\"embedding\": -0.8954028487205505, \"dimension\": 6, \"position\": 82}, {\"embedding\": -0.865545928478241, \"dimension\": 6, \"position\": 83}, {\"embedding\": -0.832244336605072, \"dimension\": 6, \"position\": 84}, {\"embedding\": -0.7956306338310242, \"dimension\": 6, \"position\": 85}, {\"embedding\": -0.7558504939079285, \"dimension\": 6, \"position\": 86}, {\"embedding\": -0.7130622863769531, \"dimension\": 6, \"position\": 87}, {\"embedding\": -0.6674363017082214, \"dimension\": 6, \"position\": 88}, {\"embedding\": -0.6191540360450745, \"dimension\": 6, \"position\": 89}, {\"embedding\": -0.5684077143669128, \"dimension\": 6, \"position\": 90}, {\"embedding\": -0.5153992176055908, \"dimension\": 6, \"position\": 91}, {\"embedding\": -0.46033963561058044, \"dimension\": 6, \"position\": 92}, {\"embedding\": -0.4034479856491089, \"dimension\": 6, \"position\": 93}, {\"embedding\": -0.34495070576667786, \"dimension\": 6, \"position\": 94}, {\"embedding\": -0.28508061170578003, \"dimension\": 6, \"position\": 95}, {\"embedding\": -0.22407597303390503, \"dimension\": 6, \"position\": 96}, {\"embedding\": -0.16217957437038422, \"dimension\": 6, \"position\": 97}, {\"embedding\": -0.09963773190975189, \"dimension\": 6, \"position\": 98}, {\"embedding\": -0.03669935464859009, \"dimension\": 6, \"position\": 99}, {\"embedding\": 1.0, \"dimension\": 7, \"position\": 0}, {\"embedding\": 0.9980100989341736, \"dimension\": 7, \"position\": 1}, {\"embedding\": 0.992048442363739, \"dimension\": 7, \"position\": 2}, {\"embedding\": 0.9821385741233826, \"dimension\": 7, \"position\": 3}, {\"embedding\": 0.9683201313018799, \"dimension\": 7, \"position\": 4}, {\"embedding\": 0.9506479501724243, \"dimension\": 7, \"position\": 5}, {\"embedding\": 0.9291924834251404, \"dimension\": 7, \"position\": 6}, {\"embedding\": 0.9040390253067017, \"dimension\": 7, \"position\": 7}, {\"embedding\": 0.8752877116203308, \"dimension\": 7, \"position\": 8}, {\"embedding\": 0.8430529832839966, \"dimension\": 7, \"position\": 9}, {\"embedding\": 0.8074631094932556, \"dimension\": 7, \"position\": 10}, {\"embedding\": 0.768659770488739, \"dimension\": 7, \"position\": 11}, {\"embedding\": 0.7267972826957703, \"dimension\": 7, \"position\": 12}, {\"embedding\": 0.6820423603057861, \"dimension\": 7, \"position\": 13}, {\"embedding\": 0.6345731019973755, \"dimension\": 7, \"position\": 14}, {\"embedding\": 0.5845783948898315, \"dimension\": 7, \"position\": 15}, {\"embedding\": 0.5322571992874146, \"dimension\": 7, \"position\": 16}, {\"embedding\": 0.47781774401664734, \"dimension\": 7, \"position\": 17}, {\"embedding\": 0.42147669196128845, \"dimension\": 7, \"position\": 18}, {\"embedding\": 0.3634582757949829, \"dimension\": 7, \"position\": 19}, {\"embedding\": 0.3039934039115906, \"dimension\": 7, \"position\": 20}, {\"embedding\": 0.24331867694854736, \"dimension\": 7, \"position\": 21}, {\"embedding\": 0.18167562782764435, \"dimension\": 7, \"position\": 22}, {\"embedding\": 0.11930955201387405, \"dimension\": 7, \"position\": 23}, {\"embedding\": 0.056468646973371506, \"dimension\": 7, \"position\": 24}, {\"embedding\": -0.006596986670047045, \"dimension\": 7, \"position\": 25}, {\"embedding\": -0.06963636726140976, \"dimension\": 7, \"position\": 26}, {\"embedding\": -0.1323986053466797, \"dimension\": 7, \"position\": 27}, {\"embedding\": -0.19463394582271576, \"dimension\": 7, \"position\": 28}, {\"embedding\": -0.25609466433525085, \"dimension\": 7, \"position\": 29}, {\"embedding\": -0.31653621792793274, \"dimension\": 7, \"position\": 30}, {\"embedding\": -0.37571802735328674, \"dimension\": 7, \"position\": 31}, {\"embedding\": -0.4334045648574829, \"dimension\": 7, \"position\": 32}, {\"embedding\": -0.4893662631511688, \"dimension\": 7, \"position\": 33}, {\"embedding\": -0.5433804392814636, \"dimension\": 7, \"position\": 34}, {\"embedding\": -0.5952320098876953, \"dimension\": 7, \"position\": 35}, {\"embedding\": -0.6447147727012634, \"dimension\": 7, \"position\": 36}, {\"embedding\": -0.6916317343711853, \"dimension\": 7, \"position\": 37}, {\"embedding\": -0.7357961535453796, \"dimension\": 7, \"position\": 38}, {\"embedding\": -0.7770323157310486, \"dimension\": 7, \"position\": 39}, {\"embedding\": -0.8151760101318359, \"dimension\": 7, \"position\": 40}, {\"embedding\": -0.8500756025314331, \"dimension\": 7, \"position\": 41}, {\"embedding\": -0.8815920352935791, \"dimension\": 7, \"position\": 42}, {\"embedding\": -0.9095999598503113, \"dimension\": 7, \"position\": 43}, {\"embedding\": -0.9339879155158997, \"dimension\": 7, \"position\": 44}, {\"embedding\": -0.9546588659286499, \"dimension\": 7, \"position\": 45}, {\"embedding\": -0.9715304374694824, \"dimension\": 7, \"position\": 46}, {\"embedding\": -0.9845356345176697, \"dimension\": 7, \"position\": 47}, {\"embedding\": -0.9936226010322571, \"dimension\": 7, \"position\": 48}, {\"embedding\": -0.998755156993866, \"dimension\": 7, \"position\": 49}, {\"embedding\": -0.9999129772186279, \"dimension\": 7, \"position\": 50}, {\"embedding\": -0.9970913529396057, \"dimension\": 7, \"position\": 51}, {\"embedding\": -0.9903015494346619, \"dimension\": 7, \"position\": 52}, {\"embedding\": -0.9795706272125244, \"dimension\": 7, \"position\": 53}, {\"embedding\": -0.9649412035942078, \"dimension\": 7, \"position\": 54}, {\"embedding\": -0.9464715719223022, \"dimension\": 7, \"position\": 55}, {\"embedding\": -0.9242352843284607, \"dimension\": 7, \"position\": 56}, {\"embedding\": -0.8983206748962402, \"dimension\": 7, \"position\": 57}, {\"embedding\": -0.8688310384750366, \"dimension\": 7, \"position\": 58}, {\"embedding\": -0.835883617401123, \"dimension\": 7, \"position\": 59}, {\"embedding\": -0.7996096611022949, \"dimension\": 7, \"position\": 60}, {\"embedding\": -0.7601534128189087, \"dimension\": 7, \"position\": 61}, {\"embedding\": -0.7176719307899475, \"dimension\": 7, \"position\": 62}, {\"embedding\": -0.6723343133926392, \"dimension\": 7, \"position\": 63}, {\"embedding\": -0.6243209838867188, \"dimension\": 7, \"position\": 64}, {\"embedding\": -0.5738229751586914, \"dimension\": 7, \"position\": 65}, {\"embedding\": -0.5210413336753845, \"dimension\": 7, \"position\": 66}, {\"embedding\": -0.4661860466003418, \"dimension\": 7, \"position\": 67}, {\"embedding\": -0.4094754755496979, \"dimension\": 7, \"position\": 68}, {\"embedding\": -0.3511352837085724, \"dimension\": 7, \"position\": 69}, {\"embedding\": -0.29139766097068787, \"dimension\": 7, \"position\": 70}, {\"embedding\": -0.23050034046173096, \"dimension\": 7, \"position\": 71}, {\"embedding\": -0.1686856895685196, \"dimension\": 7, \"position\": 72}, {\"embedding\": -0.1061997190117836, \"dimension\": 7, \"position\": 73}, {\"embedding\": -0.04329109936952591, \"dimension\": 7, \"position\": 74}, {\"embedding\": 0.019789811223745346, \"dimension\": 7, \"position\": 75}, {\"embedding\": 0.08279196172952652, \"dimension\": 7, \"position\": 76}, {\"embedding\": 0.14546461403369904, \"dimension\": 7, \"position\": 77}, {\"embedding\": 0.20755836367607117, \"dimension\": 7, \"position\": 78}, {\"embedding\": 0.26882606744766235, \"dimension\": 7, \"position\": 79}, {\"embedding\": 0.32902392745018005, \"dimension\": 7, \"position\": 80}, {\"embedding\": 0.3879123628139496, \"dimension\": 7, \"position\": 81}, {\"embedding\": 0.4452569782733917, \"dimension\": 7, \"position\": 82}, {\"embedding\": 0.5008295774459839, \"dimension\": 7, \"position\": 83}, {\"embedding\": 0.5544090270996094, \"dimension\": 7, \"position\": 84}, {\"embedding\": 0.6057820916175842, \"dimension\": 7, \"position\": 85}, {\"embedding\": 0.6547442078590393, \"dimension\": 7, \"position\": 86}, {\"embedding\": 0.7011006474494934, \"dimension\": 7, \"position\": 87}, {\"embedding\": 0.7446669340133667, \"dimension\": 7, \"position\": 88}, {\"embedding\": 0.785269558429718, \"dimension\": 7, \"position\": 89}, {\"embedding\": 0.8227470517158508, \"dimension\": 7, \"position\": 90}, {\"embedding\": 0.8569501638412476, \"dimension\": 7, \"position\": 91}, {\"embedding\": 0.8877428770065308, \"dimension\": 7, \"position\": 92}, {\"embedding\": 0.9150025844573975, \"dimension\": 7, \"position\": 93}, {\"embedding\": 0.9386208057403564, \"dimension\": 7, \"position\": 94}, {\"embedding\": 0.9585035443305969, \"dimension\": 7, \"position\": 95}, {\"embedding\": 0.974571704864502, \"dimension\": 7, \"position\": 96}, {\"embedding\": 0.9867612719535828, \"dimension\": 7, \"position\": 97}, {\"embedding\": 0.995023787021637, \"dimension\": 7, \"position\": 98}, {\"embedding\": 0.9993263483047485, \"dimension\": 7, \"position\": 99}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "chart.interactive()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b49gwRTFjob"
      },
      "source": [
        "## Encoder\n",
        "\n",
        "The original transformer consisted of 6 encoder and decoder layers. Each encoder layer has 2 parts: the Multi Head Attention layer and the feed forward layer. The outputs from the layer are fed through an additional Residual (additive) and LayerNorm layer.\n",
        "\n",
        "![Encoder flow diagram](https://github.com/ShawonAshraf/annotated-transformer-flax/blob/main/images/encoder.png?raw=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fuF4yluFjob"
      },
      "source": [
        "## Multi Head Attention\n",
        "\n",
        "A multi head attention layer consists of $n$ self-attention layers (also known as heads). Each head is expected to model a different pattern or aspect from a sequence.\n",
        "\n",
        "### Self-Attention\n",
        "\n",
        "Self-attention compares a sequence against itself and tries to figure out how each element correlates to the rest of the components (learning semantic similarities for example). As a result, for a sequence of length $N$, this operation becomes an $N \\times N$ comparison. Also, the self-attention has quadriatic memory complexity (hence all the context length limit and high gpu requirements in today's large language models).\n",
        "\n",
        "Self-attention uses query, key and values to describe its operation, which can get a bit confusing without proper semantics. To keep things simpler, consider that you're searching for information in a sequence. The query is your question or search input, the key is the available information in the sequence (consider the sequence as your search space of context of information) and values are the probability of each element in the sequence being the desired search result.\n",
        "\n",
        "(Latent spaces in the neural nets are just compressed search spaces, if you see it like that!)\n",
        "\n",
        "So for query $Q$, key $K$ and values $V$, attention $A$ is\n",
        "\n",
        "$$\n",
        "A(Q, K, V) = softmax(QK^{T})V\n",
        "$$\n",
        "\n",
        "Why softmax? V stores the probabilities. And softmax gives you a probability distribution.\n",
        "\n",
        "There's one small problem though, remember scaling the embeddings? It was done to avoid numerical overflow (quite common if your gradients suddenly explode during training). So, self-attention in transformer is also scaled by the a factor, which here is the dimension of the keys or the information space (consider this as narrowing your search space for speed).\n",
        "\n",
        "$$\n",
        "A(Q, K, V) = softmax(\\frac{QK^{T}}{\\sqrt{D_{key}}})V\n",
        "$$\n",
        "\n",
        "In a nutshell:\n",
        "\n",
        "![self attention block diagram](https://github.com/ShawonAshraf/annotated-transformer-flax/blob/main/images/attn.png?raw=1)\n",
        "\n",
        "\n",
        "For GPT like causal (or generative) models, which consist only of decoders (no encoders there!) there's a concept called masking. (Will get back to this later duding the decoder implementation!)\n",
        "\n",
        "![self attention block diagram with masking](https://github.com/ShawonAshraf/annotated-transformer-flax/blob/main/images/attn_mask.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "USSNEw-EFjob"
      },
      "outputs": [],
      "source": [
        "def self_attention(q, k, v, mask=None, dropout=None):\n",
        "    d_key = q.shape[-1]\n",
        "\n",
        "    q_k_t = jnp.matmul(q, rearrange(k, \"batch h seq k -> batch h k seq\"))\n",
        "\n",
        "    scaled = q_k_t / jnp.sqrt(d_key)\n",
        "\n",
        "    if mask:\n",
        "        # since jax numpy doesn't have a masked fill like np.ma.array\n",
        "        # https://github.com/jax-ml/jax/discussions/9363#discussioncomment-2066105\n",
        "        scaled = jnp.where(mask == 0, -1e9, x)\n",
        "\n",
        "\n",
        "    proba = nn.softmax(scaled)\n",
        "    if dropout:\n",
        "        proba = dropout(proba)\n",
        "\n",
        "    attn = jnp.matmul(proba, v)\n",
        "    return attn\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk-Vz-MKFjob"
      },
      "source": [
        "Also, why mat-mul? Faster than scaled products and can be acclelerated on GPUs and TPUs (via XLA and CUDA).\n",
        "\n",
        "Okay, now that there's one attention head defined, we can have a look at Multi-Head-Attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk8myaCeFjoc"
      },
      "source": [
        "### Attention Heads\n",
        "\n",
        "![multi head attention block](https://github.com/ShawonAshraf/annotated-transformer-flax/blob/main/images/mha.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Z4fwffNsFjoc"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    nheads: int\n",
        "    d_model: int\n",
        "    dropout_value:float = 0.1\n",
        "\n",
        "\n",
        "    def setup(self):\n",
        "        assert self.d_model % self.nheads == 0, \"Number of heads must be a multiple of d_model\"\n",
        "\n",
        "        self.d_k = self.d_model // self.nheads\n",
        "\n",
        "        # 3 dense linear layers, for q, k, v\n",
        "        self.linear_layers = [nn.Dense(self.d_model)] * 3\n",
        "        self.dropout = nn.Dropout(self.dropout_value, deterministic=True)\n",
        "\n",
        "        # one final dense layer\n",
        "        self.final_linear = nn.Dense(self.d_model)\n",
        "\n",
        "    def __call__(self, q, k, v, mask=None):\n",
        "        if mask:\n",
        "            mask = rearrange(mask, \"s -> s 1\")\n",
        "\n",
        "        # apply the dense layers\n",
        "        q, k, v = [\n",
        "            rearrange(lin(i), \"batch seq (h k) -> batch h seq k\", h=self.nheads, k=self.d_k)\n",
        "            for lin, i in zip(self.linear_layers, (q, k, v))\n",
        "        ]\n",
        "\n",
        "\n",
        "        # attention\n",
        "        score = self_attention(q, k, v, dropout=self.dropout, mask=mask)\n",
        "\n",
        "        # concat the scores\n",
        "        concatenated = jnp.concat(score)\n",
        "\n",
        "        # pass through a linear layer\n",
        "        return self.final_linear(concatenated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc-6DJR3Fjoc",
        "outputId": "e7db2c03-a41a-4c28-b625-7aef78d554e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 100, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "mha_key, master_key = jax.random.split(master_key, 2)\n",
        "\n",
        "mha = MultiHeadAttention(4, 20)\n",
        "vars = mha.init(\n",
        "    mha_key,\n",
        "    encodings,\n",
        "    encodings,\n",
        "    encodings\n",
        ")\n",
        "\n",
        "mha_out = mha.apply(\n",
        "    vars,\n",
        "    encodings,\n",
        "    encodings,\n",
        "    encodings\n",
        ")\n",
        "\n",
        "mha_out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8mAHPZoFjod"
      },
      "source": [
        "(A note to the people who wrote the updated version of the annotated transformer notebook: were you writing for an audience or just revising for an exam? Things which can be said can always be said in easier words - same goes for code, don't overly complicate variable naming and function signatures.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-Ln4iBXFjod"
      },
      "source": [
        "### Layer Normalisation\n",
        "\n",
        "When you're dealing with sequences, they can be of variable lengths. And for this, the outputs (or activations) from a layer can have different sizes. So, you can't normalise them as batch or as a group. Layer norm can handle variable length outputs from a neural network layer. (P.S. layer norm doesn't perform well for fixed sized inputs, you're better off with batch norm or group norm)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-rwUR73NFjod"
      },
      "outputs": [],
      "source": [
        "# you know, the pytorch docs were more helpful to figure this one out\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\n",
        "\n",
        "# TODO: fix layer norm\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    normalised_shape: tuple\n",
        "    eps: float = 1e-6\n",
        "\n",
        "    def setup(self):\n",
        "        # keeping bias 0\n",
        "        self.gamma = jnp.ones(shape=self.normalised_shape)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        mean = x.mean(axis=-1, keepdims=True)\n",
        "        std = x.std(axis=-1, keepdims=True)\n",
        "        norm = self.gamma * (x - mean) / (std + self.eps)\n",
        "\n",
        "        return norm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7dqqqHLFjod"
      },
      "source": [
        "### Residual + Norm Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HBBeMRHNFjod"
      },
      "outputs": [],
      "source": [
        "class AddAndNormLayer(nn.Module):\n",
        "    normalised_shape: tuple\n",
        "    dropout_rate: float\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        return x + LayerNorm(self.normalised_shape)(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ji2UCgBaFjoe",
        "outputId": "4c2a3646-0ca5-4034-fbe5-01ff2e9efdef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 100, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "norm_res_key, master_key = jax.random.split(master_key, 2)\n",
        "\n",
        "add_norm_layer = AddAndNormLayer(mha_out.shape, 0.1)\n",
        "vars = add_norm_layer.init(norm_res_key, mha_out)\n",
        "add_norm_out = add_norm_layer.apply(vars, mha_out)\n",
        "add_norm_out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9Sw4hrVFjoe"
      },
      "source": [
        "### Feed Forward Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HD3nqzcRFjoe"
      },
      "outputs": [],
      "source": [
        "class FeedForwardLayer(nn.Module):\n",
        "    d_model: int\n",
        "    hidden: int\n",
        "    dropout_rate: float = 0.1\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = nn.Dense(self.d_model, self.hidden)(x)\n",
        "        x = nn.Dense(self.hidden, self.d_model)(x)\n",
        "        x = nn.Dropout(self.dropout_rate, deterministic=True)(x)\n",
        "        # leaky_relu can be used as well!\n",
        "        x = nn.relu(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRpFj2CMFjof",
        "outputId": "8140117c-a769-4ed8-881c-d5b65548646f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 100, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "ff_key, master_key = jax.random.split(master_key, 2)\n",
        "\n",
        "ff_layer = FeedForwardLayer(20, 10)\n",
        "vars = ff_layer.init(ff_key, add_norm_out)\n",
        "ff_out = ff_layer.apply(vars, add_norm_out)\n",
        "\n",
        "ff_out.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRId3nsnFjog"
      },
      "source": [
        "### Gathering the encoder layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "iYwzOssHFjog"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    d_model: int\n",
        "    ff_hidden: int\n",
        "    dropout_rate: float = 0.1\n",
        "    nheads: int = 4\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "        x = MultiHeadAttention(self.nheads, self.d_model, self.dropout_rate)(x, x, x)\n",
        "\n",
        "        x = AddAndNormLayer(x.shape, self.dropout_rate)(x)\n",
        "\n",
        "        x = FeedForwardLayer(self.d_model, self.ff_hidden, self.dropout_rate)(x)\n",
        "\n",
        "        x = AddAndNormLayer(x.shape, self.dropout_rate)(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qm0rgzThFjog",
        "outputId": "0a24abc2-197f-45ad-e1a7-6f5733fb16de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 100, 80)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "transformer_encoder = TransformerEncoder(d_model=20, ff_hidden=80)\n",
        "\n",
        "enc_key, master_key = jax.random.split(master_key, 2)\n",
        "vars = transformer_encoder.init(enc_key, encodings)\n",
        "enc_out = transformer_encoder.apply(vars, encodings)\n",
        "enc_out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzxE5GQuFjoh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}